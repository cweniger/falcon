{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2946e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training neural network. Epochs trained: 1"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m inference \u001b[38;5;241m=\u001b[39m SNPE(prior\u001b[38;5;241m=\u001b[39mprior, density_estimator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m inference \u001b[38;5;241m=\u001b[39m inference\u001b[38;5;241m.\u001b[39mappend_simulations(theta, x)\n\u001b[0;32m---> 23\u001b[0m density_estimator \u001b[38;5;241m=\u001b[39m \u001b[43minference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m posterior_npe \u001b[38;5;241m=\u001b[39m inference\u001b[38;5;241m.\u001b[39mbuild_posterior(density_estimator)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# --- Option 2: SNPE-C (atomic loss) ---\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/emri_few/lib/python3.10/site-packages/sbi/inference/trainers/npe/npe_c.py:189\u001b[0m, in \u001b[0;36mNPE_C.train\u001b[0;34m(self, num_atoms, training_batch_size, learning_rate, validation_fraction, stop_after_epochs, max_num_epochs, clip_max_norm, calibration_kernel, resume_training, force_first_round_loss, discard_prior_samples, use_combined_loss, retrain_from_scratch, show_train_summary, dataloader_kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_non_atomic_loss:\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;66;03m# Take care of z-scoring, pre-compute and store prior terms.\u001b[39;00m\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_state_for_mog_proposal()\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/emri_few/lib/python3.10/site-packages/sbi/inference/trainers/npe/npe_base.py:375\u001b[0m, in \u001b[0;36mPosteriorEstimator.train\u001b[0;34m(self, training_batch_size, learning_rate, validation_fraction, stop_after_epochs, max_num_epochs, clip_max_norm, calibration_kernel, resume_training, force_first_round_loss, discard_prior_samples, retrain_from_scratch, show_train_summary, dataloader_kwargs)\u001b[0m\n\u001b[1;32m    373\u001b[0m     train_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clip_max_norm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 375\u001b[0m         \u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_neural_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_max_norm\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/emri_few/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py:34\u001b[0m, in \u001b[0;36m_no_grad.<locals>._no_grad_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_no_grad_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 34\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/emri_few/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py:215\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m    213\u001b[0m     parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(parameters)\n\u001b[1;32m    214\u001b[0m grads \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m--> 215\u001b[0m total_norm \u001b[38;5;241m=\u001b[39m \u001b[43m_get_total_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_if_nonfinite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m _clip_grads_with_norm_(parameters, max_norm, total_norm, foreach)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_norm\n",
      "File \u001b[0;32m~/.conda/envs/emri_few/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py:34\u001b[0m, in \u001b[0;36m_no_grad.<locals>._no_grad_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_no_grad_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 34\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/emri_few/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py:98\u001b[0m, in \u001b[0;36m_get_total_norm\u001b[0;34m(tensors, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m         norms\u001b[38;5;241m.\u001b[39mextend(\n\u001b[1;32m     94\u001b[0m             [torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mvector_norm(g, norm_type) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m device_tensors]\n\u001b[1;32m     95\u001b[0m         )\n\u001b[1;32m     97\u001b[0m total_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mvector_norm(\n\u001b[0;32m---> 98\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_device\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnorms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, norm_type\n\u001b[1;32m     99\u001b[0m )\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_if_nonfinite \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlogical_or(total_norm\u001b[38;5;241m.\u001b[39misnan(), total_norm\u001b[38;5;241m.\u001b[39misinf()):\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe total norm of order \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnorm_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for gradients from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`parameters` is non-finite, so it cannot be clipped. To disable \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis error and scale the gradients by the non-finite norm anyway, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `error_if_nonfinite=False`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sbi.inference import SNPE\n",
    "from sbi.utils import BoxUniform\n",
    "\n",
    "# Problem setup\n",
    "dim = 10\n",
    "prior = BoxUniform(low=-torch.ones(dim), high=torch.ones(dim))\n",
    "\n",
    "# Simulator: x = theta + eps\n",
    "def simulator(theta):\n",
    "    eps = 1e-4 * torch.randn_like(theta)\n",
    "    return theta + eps\n",
    "\n",
    "# Generate training data\n",
    "num_simulations = 50_000\n",
    "theta = prior.sample((num_simulations,))\n",
    "x = simulator(theta)\n",
    "\n",
    "# --- Option 1: Regular NPE (-log q(theta|x)) ---\n",
    "inference = SNPE(prior=prior, density_estimator=\"maf\")\n",
    "inference = inference.append_simulations(theta, x)\n",
    "density_estimator = inference.train()\n",
    "posterior_npe = inference.build_posterior(density_estimator)\n",
    "\n",
    "# --- Option 2: SNPE-C (atomic loss) ---\n",
    "inference_atomic = SNPE(prior=prior, density_estimator=\"maf\", loss=\"atomic\")\n",
    "inference_atomic = inference_atomic.append_simulations(theta, x)\n",
    "density_estimator_atomic = inference_atomic.train()\n",
    "posterior_atomic = inference_atomic.build_posterior(density_estimator_atomic)\n",
    "\n",
    "# Observation to infer from\n",
    "theta_true = torch.zeros(dim)  # true parameter\n",
    "x_obs = simulator(theta_true.unsqueeze(0))[0]\n",
    "\n",
    "# Sample posteriors\n",
    "samples_npe = posterior_npe.sample((10_000,), x=x_obs)\n",
    "samples_atomic = posterior_atomic.sample((10_000,), x=x_obs)\n",
    "\n",
    "# --- Plot marginals for a few dims ---\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 6))\n",
    "dims_to_plot = [0, 1, 2, 3, 4, 5]\n",
    "for ax, d in zip(axes.flat, dims_to_plot):\n",
    "    ax.hist(samples_npe[:, d].numpy(), bins=50, density=True, alpha=0.6, label=\"NPE\")\n",
    "    ax.hist(samples_atomic[:, d].numpy(), bins=50, density=True, alpha=0.6, label=\"SNPE-C\")\n",
    "    ax.axvline(theta_true[d].item(), color=\"k\", linestyle=\"--\")\n",
    "    ax.set_title(f\"θ[{d}] marginal\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a5275a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x14d7855acfa0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/weniger/.conda/envs/emri_few/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sbi.inference import SNPE\n",
    "from sbi.utils import BoxUniform\n",
    "\n",
    "# Select device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Problem setup\n",
    "dim = 10\n",
    "prior = BoxUniform(low=-torch.ones(dim, device=device),\n",
    "                   high=torch.ones(dim, device=device))\n",
    "\n",
    "# Simulator: x = theta + eps\n",
    "def simulator(theta):\n",
    "    eps = 1e-4 * torch.randn_like(theta, device=device)\n",
    "    return theta + eps\n",
    "\n",
    "# Generate training data\n",
    "num_simulations = 50_000\n",
    "theta = prior.sample((num_simulations,))\n",
    "x = simulator(theta)\n",
    "\n",
    "# --- Option 1: Regular NPE (-log q(theta|x)) ---\n",
    "inference = SNPE(prior=prior, density_estimator=\"maf\", device=device)\n",
    "inference = inference.append_simulations(theta, x)\n",
    "density_estimator = inference.train()\n",
    "posterior_npe = inference.build_posterior(density_estimator)\n",
    "\n",
    "# --- Option 2: SNPE-C (atomic loss) ---\n",
    "inference_atomic = SNPE(prior=prior, density_estimator=\"maf\", loss=\"atomic\", device=device)\n",
    "inference_atomic = inference_atomic.append_simulations(theta, x)\n",
    "density_estimator_atomic = inference_atomic.train()\n",
    "posterior_atomic = inference_atomic.build_posterior(density_estimator_atomic)\n",
    "\n",
    "# Observation to infer from\n",
    "theta_true = torch.zeros(dim, device=device)\n",
    "x_obs = simulator(theta_true.unsqueeze(0))[0]\n",
    "\n",
    "# Sample posteriors\n",
    "samples_npe = posterior_npe.sample((10_000,), x=x_obs).cpu()\n",
    "samples_atomic = posterior_atomic.sample((10_000,), x=x_obs).cpu()\n",
    "\n",
    "# --- Plot marginals for a few dims ---\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 6))\n",
    "dims_to_plot = [0, 1, 2, 3, 4, 5]\n",
    "for ax, d in zip(axes.flat, dims_to_plot):\n",
    "    ax.hist(samples_npe[:, d].numpy(), bins=50, density=True, alpha=0.6, label=\"NPE\")\n",
    "    ax.hist(samples_atomic[:, d].numpy(), bins=50, density=True, alpha=0.6, label=\"SNPE-C\")\n",
    "    ax.axvline(theta_true[d].item(), color=\"k\", linestyle=\"--\")\n",
    "    ax.set_title(f\"θ[{d}] marginal\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab52e04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emri_few",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

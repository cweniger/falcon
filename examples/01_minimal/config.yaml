# =============================================================================
# Falcon Example Configuration: 01_minimal
# -----------------------------------------------------------------------------
# A minimal working example for running a simulation-based inference (SBI)
# experiment using Falcon. This configuration defines:
#
#   • A simple 3-parameter model (z → x) using a declarative computation graph.
#   • A training buffer configuration for adaptive simulation.
#   • Integration with Weights & Biases (WandB) for experiment tracking.
#
# Structure overview:
#   logging  – WandB project setup and output directory.
#   paths    – Locations of user code and generated data.
#   buffer   – Data collection and resampling behavior.
#   graph    – Declarative model definition with simulators and estimators.
#   sample   – Sampling parameters for posterior draws after training.
#
# Usage:
#   falcon launch --run-dir outputs/my_run
#   falcon launch  # auto-generates: outputs/adj-noun-YYMMDD-HHMM
#   falcon sample posterior --run-dir outputs/my_run
#
# Notes:
#   • All outputs (simulations, trained models, logs) are written under
#     ${run_dir}, which is set via --run-dir or auto-generated.
#   • ${run_dir} is referenced by buffer, graph, and WandB logging paths.
#   • Resuming: specify existing --run-dir to load saved config.yaml
# =============================================================================

# -----------------------------------------------------------------------------
# Logging configuration
# -----------------------------------------------------------------------------
logging:
  wandb:                                  # WandB-specific settings (optional if wandb not installed)
    enabled: false                        # Set to false to disable wandb logging
    project: falcon_examples              # WandB project name
    group: 01_minimal                     # Experiment group (for organizing runs)
    dir: ${run_dir}                       # Directory for wandb files
  local:
    enabled: true                         # Local file-based logging
    dir: ${paths.graph}                   # Reuses graph directory for metrics

# -----------------------------------------------------------------------------
# Directory configuration
# -----------------------------------------------------------------------------
paths:
  import: "./src"                       # Local folder(s) with user-defined code (e.g. model.py)
  buffer: ${run_dir}/sim_dir            # Directory where Falcon stores simulated training data
  graph: ${run_dir}/graph_dir           # Directory for serialized graph and trained networks
  samples: ${run_dir}/samples_dir       # Directory for generated samples (posterior, prior, etc.)

# -----------------------------------------------------------------------------
# Training buffer parameters
# -----------------------------------------------------------------------------
buffer:
  min_training_samples: 4096            # Minimum samples before training starts
  max_training_samples: 32768           # Maximum number of samples retained in buffer
  validation_window_size: 256           # Size of validation split (for early stopping)
  resample_batch_size: 128              # Number of new samples drawn per resampling step
  keep_resampling: true                 # Continue resampling after reaching max samples
  resample_interval: 10                 # Resample every N training epochs

# -----------------------------------------------------------------------------
# Graph definition using declarative YAML
# -----------------------------------------------------------------------------
graph:
  z:
    evidence: [x]                       # Node z is inferred from observed x

    simulator:                          # Defines the prior distribution for z
      _target_: falcon.contrib.HypercubeMappingPrior
      priors:                           # Uniform priors for each parameter dimension
        - ['uniform', -100.0, 100.0]
        - ['uniform', -100.0, 100.0]
        - ['uniform', -100.0, 100.0]

    estimator:                          # Posterior estimator network
      _target_: falcon.contrib.SNPE_A   # Sequential Neural Posterior Estimation (SNPE-A)
      loop:                             # Training loop parameters
        num_epochs: 300
        batch_size: 128
        early_stop_patience: 32         # Early stopping patience
      network:                          # Neural network architecture
        net_type: nsf                   # Neural spline flow (alternatives: zuko_gf, maf, naf, etc.)
        theta_norm: true                # Normalize parameter space
        norm_momentum: 0.003            # Momentum for online normalization updates
        embedding:                      # Neural embedding for observation x
          _target_: model.E
          _input_: [x]
      optimizer:                        # Optimizer parameters
        lr: 0.01
        lr_decay_factor: 0.5            # LR decay multiplier
        scheduler_patience: 16          # LR decay after N stagnant epochs
      inference:                        # Inference and sampling parameters
        gamma: 0.5                      # Mixing coefficient for amortization weighting
        discard_samples: false
        log_ratio_threshold: -20        # Stability cutoff for log ratios

    ray:
      num_gpus: 0                       # GPU count per Ray worker (0 = CPU)

  x:
    parents: [z]                        # Node x depends on latent parameters z
    simulator:                          # Defines forward model p(x|z)
      _target_: model.Simulate
      npar: 3                           # Number of parameters to sample from z
    observed: "./data/mock_data.npz['x']"  # NPZ key extraction syntax

# -----------------------------------------------------------------------------
# Sampling configuration (posterior draws after training)
# -----------------------------------------------------------------------------
sample:
  posterior:
    n: 1000                             # Number of posterior samples to draw
    # Output: ${paths.samples}/posterior/{timestamp}/
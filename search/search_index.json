{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Falcon","text":"<p>A Python framework for simulation-based inference with adaptive learning.</p> <p>Falcon enables adaptive learning of complex conditional distributions through a declarative YAML-based approach. Built on PyTorch, Ray, and the sbi library, it provides automatic parallelization and experiment tracking.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Declarative Configuration: Define probabilistic models using intuitive YAML syntax</li> <li>Automatic Parallelization: Distributed execution via Ray actors</li> <li>Adaptive Learning: Sequential neural posterior estimation with importance sampling</li> <li>Experiment Tracking: Built-in WandB integration for monitoring training</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>graph:\n  theta:\n    evidence: [x]\n    simulator:\n      _target_: falcon.priors.Hypercube\n      priors:\n        - ['uniform', -10.0, 10.0]\n    estimator:\n      _target_: falcon.estimators.Flow\n\n  x:\n    parents: [theta]\n    simulator:\n      _target_: model.Simulator\n    observed: \"./data/obs.npz['x']\"\n</code></pre> <pre><code>falcon launch --run-dir outputs/run_01\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install falcon-sbi\n</code></pre> <p>Or install from source:</p> <pre><code>git clone https://github.com/cweniger/falcon.git\ncd falcon\npip install -e .\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Getting Started: Walk through your first Falcon project</li> <li>Configuration: Learn the YAML configuration format</li> <li>API Reference: Explore the Python API</li> </ul>"},{"location":"configuration/","title":"Configuration","text":"<p>Falcon uses YAML configuration files powered by OmegaConf. This page documents all available options.</p>"},{"location":"configuration/#configuration-sections","title":"Configuration Sections","text":""},{"location":"configuration/#logging","title":"<code>logging</code>","text":"<p>Configure experiment tracking:</p> <pre><code>logging:\n  wandb:\n    enabled: true\n    project: \"my-project\"\n    entity: \"my-team\"\n  local:\n    enabled: true\n</code></pre> Key Type Default Description <code>wandb.enabled</code> bool <code>false</code> Enable WandB logging <code>wandb.project</code> str <code>\"falcon\"</code> WandB project name <code>wandb.entity</code> str <code>null</code> WandB team/entity <code>local.enabled</code> bool <code>true</code> Enable local file logging"},{"location":"configuration/#paths","title":"<code>paths</code>","text":"<p>Configure file paths:</p> <pre><code>paths:\n  import_path: \".\"\n  buffer_dir: \"sim_dir\"\n  graph_dir: \"graph_dir\"\n  samples_dir: \"samples_dir\"\n</code></pre> Key Type Default Description <code>import_path</code> str <code>\".\"</code> Path to import custom modules <code>buffer_dir</code> str <code>\"sim_dir\"</code> Simulation buffer directory <code>graph_dir</code> str <code>\"graph_dir\"</code> Trained models directory <code>samples_dir</code> str <code>\"samples_dir\"</code> Output samples directory"},{"location":"configuration/#buffer","title":"<code>buffer</code>","text":"<p>Configure sample management:</p> <pre><code>buffer:\n  num_epochs: 500\n  min_total_samples: 1000\n  max_total_samples: 50000\n  resample_interval: 10\n  dump_interval: 50\n</code></pre> Key Type Default Description <code>num_epochs</code> int <code>500</code> Total training epochs <code>min_total_samples</code> int <code>1000</code> Minimum samples before training <code>max_total_samples</code> int <code>50000</code> Maximum samples in buffer <code>resample_interval</code> int <code>10</code> Epochs between resampling <code>dump_interval</code> int <code>50</code> Epochs between buffer dumps"},{"location":"configuration/#graph","title":"<code>graph</code>","text":"<p>Define the computational graph. Each key is a node name:</p> <pre><code>graph:\n  node_name:\n    parents: [parent1, parent2]    # Forward model dependencies\n    evidence: [evidence1]          # Inference dependencies\n    scaffolds: [scaffold1]         # Additional conditioning\n    observed: \"./path/to/data.npz\" # Observation file\n    resample: false                # Enable adaptive resampling\n\n    simulator:                     # Forward model\n      _target_: module.ClassName\n      param1: value1\n\n    estimator:                     # Posterior learner (optional)\n      _target_: falcon.estimators.Flow\n      loop:\n        num_epochs: 300\n      network:\n        net_type: nsf\n      embedding:\n        _target_: model.MyEmbedding\n        _input_: [x]\n      optimizer:\n        lr: 0.01\n      inference:\n        gamma: 0.5\n\n    ray:                          # Ray actor configuration\n      num_gpus: 0\n      num_cpus: 1\n</code></pre>"},{"location":"configuration/#node-configuration","title":"Node Configuration","text":""},{"location":"configuration/#simulator","title":"<code>simulator</code>","text":"<p>The forward model that generates samples:</p> <pre><code>simulator:\n  _target_: falcon.priors.Hypercube\n  priors:\n    - ['uniform', -10.0, 10.0]\n    - ['normal', 0.0, 1.0]\n</code></pre>"},{"location":"configuration/#estimator","title":"<code>estimator</code>","text":"<p>The posterior learner. Falcon provides two estimators:</p> <ul> <li><code>falcon.estimators.Flow</code> \u2014 Flow-based posterior estimation (recommended for most cases)</li> <li><code>falcon.estimators.Gaussian</code> \u2014 Full covariance Gaussian posterior</li> </ul> <pre><code>estimator:\n  _target_: falcon.estimators.Flow\n\n  loop:\n    num_epochs: 300\n    batch_size: 128\n    early_stop_patience: 50\n    cache_sync_every: 0\n    max_cache_samples: 0\n    cache_on_device: false\n\n  network:\n    net_type: nsf          # nsf, maf, zuko_nice, etc.\n    theta_norm: true\n\n  embedding:\n    _target_: model.Embedding\n    _input_: [x]\n\n  optimizer:\n    lr: 0.01\n    lr_decay_factor: 0.1\n    scheduler_patience: 8\n\n  inference:\n    gamma: 0.5             # Proposal width (0=posterior, 1=prior)\n    discard_samples: true\n</code></pre>"},{"location":"configuration/#ray","title":"<code>ray</code>","text":"<p>Per-node Ray resource allocation:</p> <pre><code>ray:\n  num_gpus: 1\n  num_cpus: 2\n</code></pre>"},{"location":"configuration/#global-ray-configuration","title":"Global Ray Configuration","text":"<pre><code>ray:\n  num_cpus: 8\n  num_gpus: 1\n  object_store_memory: 1000000000\n</code></pre>"},{"location":"configuration/#observation-syntax","title":"Observation Syntax","text":"<p>Load data from NPZ files with optional key extraction:</p> <pre><code># Single-key NPZ (auto-extracted)\nobserved: \"./data/obs.npz\"\n\n# Specific key extraction\nobserved: \"./data/obs.npz['x']\"\n</code></pre>"},{"location":"configuration/#overriding-configuration","title":"Overriding Configuration","text":"<p>Override any parameter via CLI:</p> <pre><code>falcon launch buffer.num_epochs=1000 graph.theta.estimator.optimizer.lr=0.001\n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide walks you through setting up and running your first Falcon project.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9+</li> <li>PyTorch 2.0+</li> </ul>"},{"location":"getting-started/#installation","title":"Installation","text":"<pre><code>pip install falcon-sbi\n</code></pre>"},{"location":"getting-started/#project-structure","title":"Project Structure","text":"<p>A typical Falcon project has this structure:</p> <pre><code>my_project/\n\u251c\u2500\u2500 config.yml      # Graph and training configuration\n\u251c\u2500\u2500 model.py         # Your simulator and embedding definitions\n\u2514\u2500\u2500 data/\n    \u2514\u2500\u2500 obs.npz      # Observed data (optional)\n</code></pre>"},{"location":"getting-started/#minimal-example","title":"Minimal Example","text":""},{"location":"getting-started/#1-define-your-simulator","title":"1. Define Your Simulator","text":"<p>Create <code>model.py</code> with a forward model:</p> <pre><code>import numpy as np\n\nclass Simulator:\n    \"\"\"Simple Gaussian simulator.\"\"\"\n\n    def sample(self, batch_dim, parent_conditions=[]):\n        theta = parent_conditions[0]  # Parameters from parent node\n        noise = np.random.randn(*theta.shape) * 0.1\n        return theta + noise\n</code></pre>"},{"location":"getting-started/#2-create-configuration","title":"2. Create Configuration","text":"<p>Create <code>config.yml</code>:</p> <pre><code>logging:\n  wandb:\n    enabled: false\n  local:\n    enabled: true\n\npaths:\n  import_path: \".\"\n\nbuffer:\n  num_epochs: 100\n  min_total_samples: 1000\n  max_total_samples: 10000\n\ngraph:\n  theta:\n    evidence: [x]\n    simulator:\n      _target_: falcon.priors.Hypercube\n      priors:\n        - ['uniform', -10.0, 10.0]\n    estimator:\n      _target_: falcon.estimators.Flow\n      loop:\n        num_epochs: 100\n      network:\n        net_type: nsf\n\n  x:\n    parents: [theta]\n    simulator:\n      _target_: model.Simulator\n    observed: \"./data/obs.npz['x']\"\n</code></pre>"},{"location":"getting-started/#3-prepare-observation-data","title":"3. Prepare Observation Data","text":"<pre><code>import numpy as np\n\n# Generate synthetic observation\ntrue_theta = 2.5\nobs = true_theta + np.random.randn(100) * 0.1\n\nnp.savez(\"data/obs.npz\", x=obs)\n</code></pre>"},{"location":"getting-started/#4-run-training","title":"4. Run Training","text":"<pre><code>falcon launch --run-dir outputs/run_01\n</code></pre>"},{"location":"getting-started/#5-sample-from-posterior","title":"5. Sample from Posterior","text":"<pre><code>falcon sample posterior --run-dir outputs/run_01\n</code></pre>"},{"location":"getting-started/#cli-commands","title":"CLI Commands","text":"Command Description <code>falcon launch</code> Start training <code>falcon sample prior</code> Sample from prior <code>falcon sample posterior</code> Sample from learned posterior <code>falcon sample proposal</code> Sample from proposal distribution <code>falcon graph</code> Display graph structure <code>falcon monitor</code> Launch TUI dashboard"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Configuration options</li> <li>Explore the API Reference</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>This section documents Falcon's Python API.</p>"},{"location":"api/#package-structure","title":"Package Structure","text":"<pre><code>falcon/\n\u251c\u2500\u2500 core/              # Core framework\n\u2502   \u251c\u2500\u2500 graph           # Graph and Node definitions\n\u2502   \u251c\u2500\u2500 deployed_graph  # Runtime execution\n\u2502   \u2514\u2500\u2500 base_estimator  # Estimator interface\n\u251c\u2500\u2500 estimators/        # Posterior estimation\n\u2502   \u251c\u2500\u2500 flow            # Flow-based posterior estimation\n\u2502   \u251c\u2500\u2500 gaussian        # Gaussian posterior estimation\n\u2502   \u2514\u2500\u2500 flow_density    # Normalizing flow networks\n\u251c\u2500\u2500 priors/            # Prior distributions\n\u2502   \u251c\u2500\u2500 hypercube       # Hypercube mapping prior\n\u2502   \u2514\u2500\u2500 product         # Product of marginals\n\u2514\u2500\u2500 embeddings/        # Observation embeddings\n    \u251c\u2500\u2500 builder         # Declarative embedding pipelines\n    \u251c\u2500\u2500 norms           # Online normalization utilities\n    \u2514\u2500\u2500 svd             # Streaming PCA\n</code></pre>"},{"location":"api/#core-classes","title":"Core Classes","text":"Class Description <code>Graph</code> Container for computational graph nodes <code>Node</code> Single random variable in the graph <code>DeployedGraph</code> Runtime orchestration with Ray <code>BaseEstimator</code> Abstract interface for estimators"},{"location":"api/#estimators","title":"Estimators","text":"Class Description <code>Flow</code> Flow-based posterior estimation (normalizing flows) <code>Gaussian</code> Full covariance Gaussian posterior <code>FlowDensity</code> Normalizing flow <code>nn.Module</code> (internal)"},{"location":"api/#priors","title":"Priors","text":"Class Description <code>Hypercube</code> Hypercube-to-target distribution mapping <code>Product</code> Product of independent marginals with latent space transforms"},{"location":"api/#embeddings","title":"Embeddings","text":"Class / Function Description <code>instantiate_embedding</code> Declarative embedding pipeline builder <code>LazyOnlineNorm</code> Online normalization <code>DiagonalWhitener</code> Diagonal whitening <code>PCAProjector</code> Streaming PCA projector"},{"location":"api/#quick-import","title":"Quick Import","text":"<pre><code>import falcon\n\n# Core\nfrom falcon import Graph, Node, CompositeNode, DeployedGraph\n\n# Estimators\nfrom falcon.estimators import Flow, Gaussian\n\n# Priors\nfrom falcon.priors import Hypercube, Product\n\n# Embeddings\nfrom falcon.embeddings import instantiate_embedding, LazyOnlineNorm, PCAProjector\n\n# Utilities\nfrom falcon import read_run, load_run, read_samples\n</code></pre>"},{"location":"api/base-estimator/","title":"BaseEstimator","text":"<p>The abstract base class defining the estimator interface.</p>"},{"location":"api/base-estimator/#overview","title":"Overview","text":"<p>All estimators in Falcon must implement this interface. The base class defines methods for:</p> <ul> <li>Training (<code>train</code>)</li> <li>Sampling (<code>sample_prior</code>, <code>sample_posterior</code>, <code>sample_proposal</code>)</li> <li>Persistence (<code>save</code>, <code>load</code>)</li> <li>Control flow (<code>pause</code>, <code>resume</code>, <code>interrupt</code>)</li> </ul>"},{"location":"api/base-estimator/#class-reference","title":"Class Reference","text":""},{"location":"api/base-estimator/#falcon.core.base_estimator.BaseEstimator","title":"BaseEstimator","text":"<p>               Bases: <code>ABC</code></p> <p>Fully abstract base class defining the estimator interface.</p> <p>All methods are abstract - no implementation details. Concrete implementations must provide all functionality.</p> <p>Conditions are passed as Dict[str, Tensor] mapping node names to values. Sampling methods return dicts with 'value' (ndarray) and optionally 'log_prob' (ndarray).</p>"},{"location":"api/base-estimator/#falcon.core.base_estimator.BaseEstimator.train","title":"train  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>train(buffer)\n</code></pre> <p>Train the estimator.</p> <p>Parameters:</p> Name Type Description Default <code>buffer</code> <p>BufferView providing access to training/validation data</p> required Source code in <code>falcon/core/base_estimator.py</code> <pre><code>@abstractmethod\nasync def train(self, buffer) -&gt; None:\n    \"\"\"\n    Train the estimator.\n\n    Args:\n        buffer: BufferView providing access to training/validation data\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/base-estimator/#falcon.core.base_estimator.BaseEstimator.sample_prior","title":"sample_prior  <code>abstractmethod</code>","text":"<pre><code>sample_prior(num_samples, conditions=None)\n</code></pre> <p>Sample from the prior distribution.</p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <code>int</code> <p>Number of samples to generate</p> required <code>conditions</code> <code>Optional[Conditions]</code> <p>Conditioning values from parent nodes (usually None for prior)</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dict with 'value' (ndarray) and optionally 'log_prob' (ndarray)</p> Source code in <code>falcon/core/base_estimator.py</code> <pre><code>@abstractmethod\ndef sample_prior(\n    self, num_samples: int, conditions: Optional[Conditions] = None\n) -&gt; dict:\n    \"\"\"\n    Sample from the prior distribution.\n\n    Args:\n        num_samples: Number of samples to generate\n        conditions: Conditioning values from parent nodes (usually None for prior)\n\n    Returns:\n        Dict with 'value' (ndarray) and optionally 'log_prob' (ndarray)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/base-estimator/#falcon.core.base_estimator.BaseEstimator.sample_posterior","title":"sample_posterior  <code>abstractmethod</code>","text":"<pre><code>sample_posterior(num_samples, conditions=None)\n</code></pre> <p>Sample from the posterior distribution.</p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <code>int</code> <p>Number of samples to generate</p> required <code>conditions</code> <code>Optional[Conditions]</code> <p>Dict mapping node names to condition tensors</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dict with 'value' (ndarray) and optionally 'log_prob' (ndarray)</p> Source code in <code>falcon/core/base_estimator.py</code> <pre><code>@abstractmethod\ndef sample_posterior(\n    self, num_samples: int, conditions: Optional[Conditions] = None\n) -&gt; dict:\n    \"\"\"\n    Sample from the posterior distribution.\n\n    Args:\n        num_samples: Number of samples to generate\n        conditions: Dict mapping node names to condition tensors\n\n    Returns:\n        Dict with 'value' (ndarray) and optionally 'log_prob' (ndarray)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/base-estimator/#falcon.core.base_estimator.BaseEstimator.sample_proposal","title":"sample_proposal  <code>abstractmethod</code>","text":"<pre><code>sample_proposal(num_samples, conditions=None)\n</code></pre> <p>Sample from the proposal distribution for adaptive resampling.</p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <code>int</code> <p>Number of samples to generate</p> required <code>conditions</code> <code>Optional[Conditions]</code> <p>Dict mapping node names to condition tensors</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dict with 'value' (ndarray) and optionally 'log_prob' (ndarray)</p> Source code in <code>falcon/core/base_estimator.py</code> <pre><code>@abstractmethod\ndef sample_proposal(\n    self, num_samples: int, conditions: Optional[Conditions] = None\n) -&gt; dict:\n    \"\"\"\n    Sample from the proposal distribution for adaptive resampling.\n\n    Args:\n        num_samples: Number of samples to generate\n        conditions: Dict mapping node names to condition tensors\n\n    Returns:\n        Dict with 'value' (ndarray) and optionally 'log_prob' (ndarray)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/base-estimator/#falcon.core.base_estimator.BaseEstimator.save","title":"save  <code>abstractmethod</code>","text":"<pre><code>save(node_dir)\n</code></pre> <p>Save estimator state to directory.</p> <p>Parameters:</p> Name Type Description Default <code>node_dir</code> <code>Path</code> <p>Directory to save state to</p> required Source code in <code>falcon/core/base_estimator.py</code> <pre><code>@abstractmethod\ndef save(self, node_dir: Path) -&gt; None:\n    \"\"\"\n    Save estimator state to directory.\n\n    Args:\n        node_dir: Directory to save state to\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/base-estimator/#falcon.core.base_estimator.BaseEstimator.load","title":"load  <code>abstractmethod</code>","text":"<pre><code>load(node_dir)\n</code></pre> <p>Load estimator state from directory.</p> <p>Parameters:</p> Name Type Description Default <code>node_dir</code> <code>Path</code> <p>Directory to load state from</p> required Source code in <code>falcon/core/base_estimator.py</code> <pre><code>@abstractmethod\ndef load(self, node_dir: Path) -&gt; None:\n    \"\"\"\n    Load estimator state from directory.\n\n    Args:\n        node_dir: Directory to load state from\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/base-estimator/#falcon.core.base_estimator.BaseEstimator.pause","title":"pause  <code>abstractmethod</code>","text":"<pre><code>pause()\n</code></pre> <p>Pause training loop.</p> Source code in <code>falcon/core/base_estimator.py</code> <pre><code>@abstractmethod\ndef pause(self) -&gt; None:\n    \"\"\"Pause training loop.\"\"\"\n    pass\n</code></pre>"},{"location":"api/base-estimator/#falcon.core.base_estimator.BaseEstimator.resume","title":"resume  <code>abstractmethod</code>","text":"<pre><code>resume()\n</code></pre> <p>Resume training loop.</p> Source code in <code>falcon/core/base_estimator.py</code> <pre><code>@abstractmethod\ndef resume(self) -&gt; None:\n    \"\"\"Resume training loop.\"\"\"\n    pass\n</code></pre>"},{"location":"api/base-estimator/#falcon.core.base_estimator.BaseEstimator.interrupt","title":"interrupt  <code>abstractmethod</code>","text":"<pre><code>interrupt()\n</code></pre> <p>Terminate training loop.</p> Source code in <code>falcon/core/base_estimator.py</code> <pre><code>@abstractmethod\ndef interrupt(self) -&gt; None:\n    \"\"\"Terminate training loop.\"\"\"\n    pass\n</code></pre>"},{"location":"api/deployed-graph/","title":"DeployedGraph","text":"<p>The <code>DeployedGraph</code> class orchestrates distributed execution of Falcon graphs using Ray.</p>"},{"location":"api/deployed-graph/#overview","title":"Overview","text":"<p><code>DeployedGraph</code> wraps a <code>Graph</code> and handles:</p> <ul> <li>Ray actor initialization for each node</li> <li>Distributed sample generation and training</li> <li>Coordination between nodes during inference</li> </ul>"},{"location":"api/deployed-graph/#class-reference","title":"Class Reference","text":""},{"location":"api/deployed-graph/#falcon.core.deployed_graph.DeployedGraph","title":"DeployedGraph","text":"<pre><code>DeployedGraph(graph, model_path=None, log_config=None)\n</code></pre> <p>Initialize a DeployedGraph with the given conceptual graph of nodes.</p> <p>Note: This class uses falcon.info(), falcon.warning() etc. for logging. These functions use the module-level logger set by cli.py via set_logger().</p> Source code in <code>falcon/core/deployed_graph.py</code> <pre><code>def __init__(self, graph, model_path=None, log_config=None):\n    \"\"\"Initialize a DeployedGraph with the given conceptual graph of nodes.\n\n    Note: This class uses falcon.info(), falcon.warning() etc. for logging.\n    These functions use the module-level logger set by cli.py via set_logger().\n    \"\"\"\n    self.graph = graph\n    self.model_path = model_path\n    self.log_config = log_config or {}\n    self.wrapped_nodes_dict = {}\n    self.monitor_bridge = None\n\n    self._create_monitor_bridge()\n    self.deploy_nodes()\n</code></pre>"},{"location":"api/deployed-graph/#falcon.core.deployed_graph.DeployedGraph.deploy_nodes","title":"deploy_nodes","text":"<pre><code>deploy_nodes()\n</code></pre> <p>Deploy all nodes in the graph as Ray actors.</p> Source code in <code>falcon/core/deployed_graph.py</code> <pre><code>def deploy_nodes(self):\n    \"\"\"Deploy all nodes in the graph as Ray actors.\"\"\"\n    info(\"Spinning up graph...\")\n\n    # Create all actors (non-blocking)\n    for node in self.graph.node_list:\n        if node.num_actors &gt; 1:\n            self.wrapped_nodes_dict[node.name] = MultiplexNodeWrapper.remote(\n                node.actor_config,\n                node,\n                self.graph,\n                node.num_actors,\n                self.model_path,\n                self.log_config,\n            )\n        else:\n            self.wrapped_nodes_dict[node.name] = NodeWrapper.options(\n                **_ray_options(node.actor_config)\n            ).remote(node, self.graph, self.model_path, self.log_config)\n\n    # Wait for all actors to initialize and register with monitor bridge\n    for name, actor in self.wrapped_nodes_dict.items():\n        try:\n            # MultiplexNodeWrapper has wait_ready(), NodeWrapper uses __ray_ready__\n            if hasattr(actor, 'wait_ready'):\n                ray.get(actor.wait_ready.remote())\n            else:\n                ray.get(actor.__ray_ready__.remote())\n            info(f\"  \u2713 {name}\")\n\n            # Register node with monitor bridge\n            if self.monitor_bridge:\n                ray.get(self.monitor_bridge.register_node.remote(name, actor))\n        except ray.exceptions.RayActorError as e:\n            raise RuntimeError(f\"Failed to initialize node '{name}': {e}\") from e\n</code></pre>"},{"location":"api/deployed-graph/#falcon.core.deployed_graph.DeployedGraph.sample","title":"sample","text":"<pre><code>sample(num_samples, conditions=None)\n</code></pre> <p>Run forward sampling through the graph.</p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <p>Number of samples to generate</p> required <code>conditions</code> <p>Optional dict of pre-set conditions (arrays/tensors)</p> <code>None</code> <p>Returns:</p> Type Description <p>List[Dict[str, ObjectRef]]: One dict per sample with refs to all node values</p> Source code in <code>falcon/core/deployed_graph.py</code> <pre><code>def sample(self, num_samples, conditions=None):\n    \"\"\"Run forward sampling through the graph.\n\n    Args:\n        num_samples: Number of samples to generate\n        conditions: Optional dict of pre-set conditions (arrays/tensors)\n\n    Returns:\n        List[Dict[str, ObjectRef]]: One dict per sample with refs to all node values\n    \"\"\"\n    condition_refs = self._arrays_to_condition_refs(conditions, num_samples) if conditions else {}\n    return self._execute_graph(\n        num_samples, self.graph.sorted_node_names, condition_refs, \"sample\",\n    )\n</code></pre>"},{"location":"api/deployed-graph/#falcon.core.deployed_graph.DeployedGraph.sample_posterior","title":"sample_posterior","text":"<pre><code>sample_posterior(num_samples, conditions=None)\n</code></pre> <p>Run posterior sampling through the inference graph.</p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <p>Number of samples to generate</p> required <code>conditions</code> <p>Optional dict of pre-set conditions (arrays/tensors)</p> <code>None</code> <p>Returns:</p> Type Description <p>List[Dict[str, ObjectRef]]: One dict per sample with refs to all node values</p> Source code in <code>falcon/core/deployed_graph.py</code> <pre><code>def sample_posterior(self, num_samples, conditions=None):\n    \"\"\"Run posterior sampling through the inference graph.\n\n    Args:\n        num_samples: Number of samples to generate\n        conditions: Optional dict of pre-set conditions (arrays/tensors)\n\n    Returns:\n        List[Dict[str, ObjectRef]]: One dict per sample with refs to all node values\n    \"\"\"\n    condition_refs = self._arrays_to_condition_refs(conditions, num_samples) if conditions else {}\n    return self._execute_graph(\n        num_samples, self.graph.sorted_inference_node_names, condition_refs, \"sample_posterior\",\n    )\n</code></pre>"},{"location":"api/deployed-graph/#falcon.core.deployed_graph.DeployedGraph.sample_proposal","title":"sample_proposal","text":"<pre><code>sample_proposal(num_samples, conditions=None)\n</code></pre> <p>Run proposal sampling through the inference graph.</p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <p>Number of samples to generate</p> required <code>conditions</code> <p>Optional dict of pre-set conditions (arrays/tensors)</p> <code>None</code> <p>Returns:</p> Type Description <p>List[Dict[str, ObjectRef]]: One dict per sample with refs to all node values</p> Source code in <code>falcon/core/deployed_graph.py</code> <pre><code>def sample_proposal(self, num_samples, conditions=None):\n    \"\"\"Run proposal sampling through the inference graph.\n\n    Args:\n        num_samples: Number of samples to generate\n        conditions: Optional dict of pre-set conditions (arrays/tensors)\n\n    Returns:\n        List[Dict[str, ObjectRef]]: One dict per sample with refs to all node values\n    \"\"\"\n    condition_refs = self._arrays_to_condition_refs(conditions, num_samples) if conditions else {}\n    return self._execute_graph(\n        num_samples, self.graph.sorted_inference_node_names, condition_refs, \"sample_proposal\",\n    )\n</code></pre>"},{"location":"api/deployed-graph/#falcon.core.deployed_graph.DeployedGraph.shutdown","title":"shutdown","text":"<pre><code>shutdown()\n</code></pre> <p>Shut down the deployed graph and release resources.</p> Source code in <code>falcon/core/deployed_graph.py</code> <pre><code>def shutdown(self):\n    \"\"\"Shut down the deployed graph and release resources.\"\"\"\n    ray.get([node.shutdown.remote() for node in self.wrapped_nodes_dict.values()])\n</code></pre>"},{"location":"api/deployed-graph/#falcon.core.deployed_graph.DeployedGraph.launch","title":"launch","text":"<pre><code>launch(dataset_manager, observations, graph_path=None, stop_check=None)\n</code></pre> <p>Launch training.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_manager</code> <p>Dataset manager for samples</p> required <code>observations</code> <p>Observation data</p> required <code>graph_path</code> <p>Path to save/load graph</p> <code>None</code> <code>stop_check</code> <p>Optional callable that returns True when graceful stop is requested</p> <code>None</code> Source code in <code>falcon/core/deployed_graph.py</code> <pre><code>def launch(self, dataset_manager, observations, graph_path=None, stop_check=None):\n    \"\"\"Launch training.\n\n    Args:\n        dataset_manager: Dataset manager for samples\n        observations: Observation data\n        graph_path: Path to save/load graph\n        stop_check: Optional callable that returns True when graceful stop is requested\n    \"\"\"\n    asyncio.run(self._launch(dataset_manager, observations, graph_path=graph_path, stop_check=stop_check))\n</code></pre>"},{"location":"api/deployed-graph/#falcon.core.deployed_graph.DeployedGraph.save","title":"save","text":"<pre><code>save(graph_dir)\n</code></pre> <p>Save the deployed graph node status.</p> Source code in <code>falcon/core/deployed_graph.py</code> <pre><code>def save(self, graph_dir):\n    \"\"\"Save the deployed graph node status.\"\"\"\n    graph_dir = graph_dir.expanduser().resolve()\n    graph_dir.mkdir(parents=True, exist_ok=True)\n    save_futures = []\n    for name, node in self.wrapped_nodes_dict.items():\n        node_dir = graph_dir / name\n        save_future = node.save.remote(node_dir)\n        save_futures.append(save_future)\n    ray.get(save_futures)\n</code></pre>"},{"location":"api/deployed-graph/#falcon.core.deployed_graph.DeployedGraph.load","title":"load","text":"<pre><code>load(graph_dir)\n</code></pre> <p>Load the deployed graph nodes status.</p> Source code in <code>falcon/core/deployed_graph.py</code> <pre><code>def load(self, graph_dir):\n    \"\"\"Load the deployed graph nodes status.\"\"\"\n    info(f\"Loading deployed graph from: {graph_dir}\")\n    load_futures = []\n    for name, node in self.wrapped_nodes_dict.items():\n        node_dir = Path(graph_dir) / name\n        load_future = node.load.remote(node_dir)\n        load_futures.append(load_future)\n    ray.get(load_futures)\n</code></pre>"},{"location":"api/deployed-graph/#falcon.core.deployed_graph.DeployedGraph.pause","title":"pause","text":"<pre><code>pause()\n</code></pre> <p>Pause all nodes in the deployed graph.</p> Source code in <code>falcon/core/deployed_graph.py</code> <pre><code>def pause(self):\n    \"\"\"Pause all nodes in the deployed graph.\"\"\"\n    pause_futures = []\n    for _, node in self.wrapped_nodes_dict.items():\n        pause_future = node.pause.remote()\n        pause_futures.append(pause_future)\n    ray.get(pause_futures)\n</code></pre>"},{"location":"api/deployed-graph/#falcon.core.deployed_graph.DeployedGraph.resume","title":"resume","text":"<pre><code>resume()\n</code></pre> <p>Resume all nodes in the deployed graph.</p> Source code in <code>falcon/core/deployed_graph.py</code> <pre><code>def resume(self):\n    \"\"\"Resume all nodes in the deployed graph.\"\"\"\n    resume_futures = []\n    for _, node in self.wrapped_nodes_dict.items():\n        resume_future = node.resume.remote()\n        resume_futures.append(resume_future)\n    ray.get(resume_futures)\n</code></pre>"},{"location":"api/deployed-graph/#falcon.core.deployed_graph.DeployedGraph.interrupt","title":"interrupt","text":"<pre><code>interrupt()\n</code></pre> <p>Interrupt all nodes in the deployed graph.</p> Source code in <code>falcon/core/deployed_graph.py</code> <pre><code>def interrupt(self):\n    \"\"\"Interrupt all nodes in the deployed graph.\"\"\"\n    interrupt_futures = []\n    for _, node in self.wrapped_nodes_dict.items():\n        interrupt_future = node.interrupt.remote()\n        interrupt_futures.append(interrupt_future)\n    ray.get(interrupt_futures)\n</code></pre>"},{"location":"api/embeddings/","title":"Embeddings","text":"<p>Declarative embedding pipelines for observation processing.</p>"},{"location":"api/embeddings/#overview","title":"Overview","text":"<p>The <code>falcon.embeddings</code> package provides tools for building observation embedding networks via declarative YAML configuration. Embeddings map raw observations to lower-dimensional summary statistics before they enter the estimator.</p>"},{"location":"api/embeddings/#declarative-configuration","title":"Declarative Configuration","text":"<p>Embedding networks are defined in YAML using the <code>_target_</code> / <code>_input_</code> system:</p> <ul> <li><code>_target_</code>: Import path of the <code>nn.Module</code> class to instantiate</li> <li><code>_input_</code>: List of observation node names (or nested sub-embeddings) that feed into this module</li> </ul>"},{"location":"api/embeddings/#basic-embedding","title":"Basic Embedding","text":"<pre><code>embedding:\n  _target_: model.MyEmbedding\n  _input_: [x]\n</code></pre>"},{"location":"api/embeddings/#multi-input-embedding","title":"Multi-Input Embedding","text":"<pre><code>embedding:\n  _target_: model.MyEmbedding\n  _input_: [x, y]  # Multiple observation nodes\n</code></pre>"},{"location":"api/embeddings/#nested-pipeline","title":"Nested Pipeline","text":"<p>Sub-embeddings can be nested arbitrarily deep. Each <code>_input_</code> entry can itself be a <code>_target_</code> / <code>_input_</code> block:</p> <pre><code>embedding:\n  _target_: model.Concatenate\n  _input_:\n    - _target_: timm.create_model\n      model_name: resnet18\n      pretrained: true\n      num_classes: 0\n      _input_:\n        _target_: model.Unsqueeze\n        _input_: [image]\n    - _target_: torch.nn.Linear\n      in_features: 64\n      out_features: 32\n      _input_: [metadata]\n</code></pre>"},{"location":"api/embeddings/#built-in-utilities","title":"Built-in Utilities","text":""},{"location":"api/embeddings/#normalization","title":"Normalization","text":""},{"location":"api/embeddings/#lazyonlinenorm","title":"LazyOnlineNorm","text":"<p>Online normalization with lazy initialization and optional momentum adaptation. Normalizes inputs to zero mean and unit variance using exponential moving averages.</p> <pre><code>embedding:\n  _target_: falcon.embeddings.LazyOnlineNorm\n  _input_: [x]\n  momentum: 0.01\n</code></pre>"},{"location":"api/embeddings/#diagonalwhitener","title":"DiagonalWhitener","text":"<p>Diagonal whitening with optional Hartley transform preprocessing and momentum-based running statistics.</p> <pre><code>embedding:\n  _target_: falcon.embeddings.DiagonalWhitener\n  _input_: [x]\n</code></pre>"},{"location":"api/embeddings/#hartley_transform","title":"hartley_transform","text":"<p>A standalone function for computing the discrete Hartley transform, useful as a preprocessing step.</p>"},{"location":"api/embeddings/#dimensionality-reduction","title":"Dimensionality Reduction","text":""},{"location":"api/embeddings/#pcaprojector","title":"PCAProjector","text":"<p>Streaming dual PCA projector with momentum-based updates. Performs online SVD for dimensionality reduction with variance-based prior (ridge-like regularization) and optional output normalization.</p> <pre><code>embedding:\n  _target_: falcon.embeddings.PCAProjector\n  _input_: [x]\n  n_components: 32\n</code></pre>"},{"location":"api/embeddings/#class-reference","title":"Class Reference","text":""},{"location":"api/embeddings/#falcon.embeddings.builder.instantiate_embedding","title":"instantiate_embedding","text":"<pre><code>instantiate_embedding(embedding_config)\n</code></pre> <p>Instantiate embedding pipeline from config.</p> Source code in <code>falcon/embeddings/builder.py</code> <pre><code>def instantiate_embedding(embedding_config: Dict[str, Any]) -&gt; EmbeddingWrapper:\n    \"\"\"Instantiate embedding pipeline from config.\"\"\"\n    required_input_keys = _collect_input_keys(embedding_config)\n    modules, input_keys_list, output_keys, _ = _flatten_config_to_modules(\n        embedding_config\n    )\n    return EmbeddingWrapper(modules, input_keys_list, output_keys, required_input_keys)\n</code></pre>"},{"location":"api/embeddings/#falcon.embeddings.builder.EmbeddingWrapper","title":"EmbeddingWrapper","text":"<pre><code>EmbeddingWrapper(modules, input_keys_list, output_keys, required_input_keys)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Sequential execution of modules with shared data dictionary.</p> Source code in <code>falcon/embeddings/builder.py</code> <pre><code>def __init__(\n    self,\n    modules: List[nn.Module],\n    input_keys_list: List[List[str]],\n    output_keys: List[str],\n    required_input_keys: List[str],\n):\n    super().__init__()\n    self.modules_list = nn.ModuleList(modules)\n    self.input_keys_list = input_keys_list\n    self.output_keys = output_keys\n    self.input_keys = required_input_keys\n</code></pre>"},{"location":"api/embeddings/#falcon.embeddings.norms.LazyOnlineNorm","title":"LazyOnlineNorm","text":"<pre><code>LazyOnlineNorm(momentum=0.01, epsilon=1e-20, log_prefix=None, adaptive_momentum=False, monotonic_variance=True, use_log_update=False)\n</code></pre> <p>               Bases: <code>Module</code></p> Source code in <code>falcon/embeddings/norms.py</code> <pre><code>def __init__(\n    self,\n    momentum=0.01,\n    epsilon=1e-20,\n    log_prefix=None,\n    adaptive_momentum=False,\n    monotonic_variance=True,\n    use_log_update=False,\n):\n    super().__init__()\n    self.momentum = momentum\n    self.epsilon = epsilon\n    self.log_prefix = log_prefix\n    #self.log_prefix = log_prefix + \":\" if log_prefix else \"\"\n    self.monotonic_variance = monotonic_variance\n    self.use_log_update = use_log_update\n    self.adaptive_momentum = adaptive_momentum\n\n    self.register_buffer(\"running_mean\", None)\n    self.register_buffer(\"running_var\", None)\n    self.register_buffer(\"min_variance\", None)\n\n    self.initialized = False\n</code></pre>"},{"location":"api/embeddings/#falcon.embeddings.norms.DiagonalWhitener","title":"DiagonalWhitener","text":"<pre><code>DiagonalWhitener(dim, momentum=0.1, eps=1e-08, use_fourier=False, track_mean=True)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>dim: number of features (last dimension of x) momentum: how much of the new batch stats to use (PyTorch-style) eps: small constant for numerical stability use_fourier: if True, apply Hartley transform before whitening</p> Source code in <code>falcon/embeddings/norms.py</code> <pre><code>def __init__(self, dim, momentum=0.1, eps=1e-8, use_fourier=False, track_mean=True):\n    \"\"\"\n    dim: number of features (last dimension of x)\n    momentum: how much of the new batch stats to use (PyTorch-style)\n    eps: small constant for numerical stability\n    use_fourier: if True, apply Hartley transform before whitening\n    \"\"\"\n    super().__init__()\n    self.dim = dim\n    self.momentum = momentum\n    self.eps = eps\n    self.use_fourier = use_fourier\n    self.track_mean = track_mean\n\n    self.register_buffer(\"running_mean\", torch.zeros(dim))\n    self.register_buffer(\"running_var\", torch.ones(dim))\n    self.initialized = False\n</code></pre>"},{"location":"api/embeddings/#falcon.embeddings.norms.DiagonalWhitener.update","title":"update","text":"<pre><code>update(x)\n</code></pre> <p>Update running mean and variance from current batch x: Tensor of shape (batch_size, dim)</p> Source code in <code>falcon/embeddings/norms.py</code> <pre><code>def update(self, x):\n    \"\"\"\n    Update running mean and variance from current batch\n    x: Tensor of shape (batch_size, dim)\n    \"\"\"\n    if self.use_fourier:\n        x = hartley_transform(x)\n\n    batch_mean = x.mean(dim=0)\n    batch_var = x.var(dim=0, unbiased=False)\n\n    if not self.initialized:\n        self.running_mean = batch_mean.detach()\n        self.running_var = batch_var.detach()\n        self.initialized = True\n    else:\n        self.running_mean = (\n            1 - self.momentum\n        ) * self.running_mean + self.momentum * batch_mean.detach()\n        self.running_var = (\n            1 - self.momentum\n        ) * self.running_var + self.momentum * batch_var.detach()\n    if not self.track_mean:\n        self.running_mean *= 0\n</code></pre>"},{"location":"api/embeddings/#falcon.embeddings.norms.DiagonalWhitener.__call__","title":"__call__","text":"<pre><code>__call__(x)\n</code></pre> <p>Apply whitening: (x - mean) / std If use_fourier, whitening happens in Hartley space and is transformed back.</p> Source code in <code>falcon/embeddings/norms.py</code> <pre><code>def __call__(self, x):\n    \"\"\"\n    Apply whitening: (x - mean) / std\n    If use_fourier, whitening happens in Hartley space and is transformed back.\n    \"\"\"\n    if self.use_fourier:\n        x = hartley_transform(x)\n\n    std = torch.sqrt(self.running_var + self.eps)\n    x_white = (x - self.running_mean) / std\n\n    if self.use_fourier:\n        x_white = hartley_transform(x_white)\n\n    return x_white\n</code></pre>"},{"location":"api/embeddings/#falcon.embeddings.norms.hartley_transform","title":"hartley_transform","text":"<pre><code>hartley_transform(x)\n</code></pre> <p>Hartley transform: H(x) = Re(FFT(x)) - Im(FFT(x)) It is its own inverse: H(H(x)) = x</p> Source code in <code>falcon/embeddings/norms.py</code> <pre><code>def hartley_transform(x):\n    \"\"\"\n    Hartley transform: H(x) = Re(FFT(x)) - Im(FFT(x))\n    It is its own inverse: H(H(x)) = x\n    \"\"\"\n    fft = torch.fft.fft(x, dim=-1, norm=\"ortho\")\n    return fft.real - fft.imag\n</code></pre>"},{"location":"api/embeddings/#falcon.embeddings.svd.PCAProjector","title":"PCAProjector","text":"<pre><code>PCAProjector(n_components=10, oversampling=10, buffer_size=256, momentum=0.1, normalize_output=True, use_prior=True)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>A streaming, dual PCA projector with momentum-based updates.</p> <p>This class maintains a running mean and a set of principal components (eigenvectors) and eigenvalues (variances) of the input data, computed using a dual PCA approach. It stores incoming data in a buffer until the buffer is full, then performs an eigen-decomposition update. A momentum term blends the new PCA decomposition with the existing one to adapt over time.</p> Optionally <ul> <li>A \"prior\" can be applied, which acts like ridge (Tikhonov) regularization.   It assumes white noise on the inputs and shrinks each principal component   proportionally to 1 / (1 + 1 / eigenvalue).</li> <li>The output can be normalized so that the expected variance (averaged over   all features) is unity.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>n_components</code> <code>int</code> <p>Number of principal components to retain.</p> <code>10</code> <code>oversampling</code> <code>int</code> <p>Extra dimensions to capture a slightly larger subspace (not currently used).</p> <code>10</code> <code>buffer_size</code> <code>int</code> <p>Number of samples to accumulate before performing an SVD update.</p> <code>256</code> <code>momentum</code> <code>float</code> <p>Blend factor for merging new PCA decomposition with the old one.</p> <code>0.1</code> <code>normalize_output</code> <code>bool</code> <p>Whether to normalize the reconstructed outputs               so that they have unit average variance.</p> <code>True</code> <code>use_prior</code> <code>bool</code> <p>Whether to apply a variance-based prior (ridge-like shrinkage).</p> <code>True</code> Source code in <code>falcon/embeddings/svd.py</code> <pre><code>def __init__(\n    self,\n    n_components: int = 10,\n    # input_dim: int,\n    oversampling: int = 10,\n    buffer_size: int = 256,\n    momentum: float = 0.1,\n    normalize_output: bool = True,\n    use_prior: bool = True,\n    # add_mean: bool = False,\n) -&gt; None:\n    \"\"\"\n    Args:\n        n_components: Number of principal components to retain.\n        oversampling: Extra dimensions to capture a slightly larger subspace (not currently used).\n        buffer_size: Number of samples to accumulate before performing an SVD update.\n        momentum: Blend factor for merging new PCA decomposition with the old one.\n        normalize_output: Whether to normalize the reconstructed outputs\n                          so that they have unit average variance.\n        use_prior: Whether to apply a variance-based prior (ridge-like shrinkage).\n    \"\"\"\n    super().__init__()\n    self.n_components: int = n_components\n    # self.input_dim: int = input_dim\n    self.oversampling: int = oversampling\n    self.buffer_size: int = buffer_size\n    # self.device: str = device\n    self.momentum: float = momentum\n    self.normalize_output: bool = normalize_output\n    self.use_prior: bool = use_prior\n    # self.add_mean: bool = add_mean\n\n    # Running mean of the input data, updated incrementally\n    # self.mean: Optional[torch.Tensor] = None  # shape: (D,)\n    # Number of samples accumulated so far (used for updating the mean)\n    # self.n_samples: int = 0\n\n    # Temporary buffer for incoming data points\n    self.buffer: List[torch.Tensor] = []\n    # Counts how many samples have been appended to the buffer\n    self.buffer_counter: int = 0\n\n    # Principal components (top-k right singular vectors) and eigenvalues\n    self.components: Optional[torch.Tensor] = None  # shape: (k, D)\n    self.eigenvalues: Optional[torch.Tensor] = None  # shape: (k,)\n</code></pre>"},{"location":"api/embeddings/#falcon.embeddings.svd.PCAProjector.update","title":"update","text":"<pre><code>update(X)\n</code></pre> <p>Accumulate a batch of data in the buffer. If the buffer is full, update the PCA decomposition.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>A batch of input data with shape (batch_size, D).</p> required Source code in <code>falcon/embeddings/svd.py</code> <pre><code>def update(self, X: torch.Tensor) -&gt; None:\n    \"\"\"\n    Accumulate a batch of data in the buffer. If the buffer is full, update the PCA decomposition.\n\n    Args:\n        X: A batch of input data with shape (batch_size, D).\n    \"\"\"\n    batch_size = X.shape[0]\n\n    # Store in the buffer\n    self.buffer.append(X)\n    self.buffer_counter += batch_size\n\n    # If we've accumulated enough samples in the buffer, update the PCA decomposition\n    if self.buffer_counter &gt;= self.buffer_size:\n        self._compute_svd_update()\n        # Clear the buffer and reset counter\n        self.buffer = []\n        self.buffer_counter = 0\n</code></pre>"},{"location":"api/embeddings/#falcon.embeddings.svd.PCAProjector.forward","title":"forward","text":"<pre><code>forward(X)\n</code></pre> <p>Filter the input data by projecting onto the learned principal components, optionally applying a variance-based prior, then reconstructing and possibly normalizing.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>A batch of input data with shape (batch_size, D).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A batch of data with shape (batch_size, D) after PCA</p> <code>Tensor</code> <p>transform (and optional prior &amp; normalization).</p> Source code in <code>falcon/embeddings/svd.py</code> <pre><code>def forward(self, X: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Filter the input data by projecting onto the learned principal components,\n    optionally applying a variance-based prior, then reconstructing and possibly normalizing.\n\n    Args:\n        X: A batch of input data with shape (batch_size, D).\n\n    Returns:\n        torch.Tensor: A batch of data with shape (batch_size, D) after PCA\n        transform (and optional prior &amp; normalization).\n    \"\"\"\n    # If no PCA has been computed yet, we can't project\n    if self.components is None:\n        raise ValueError(\n            \"SVD components not computed yet. Call update() enough times first.\"\n        )\n\n    # Shift input by the running mean\n    # X_centered = X - self.mean\n    # Project onto the principal components\n    X_proj = X @ self.components.T  # shape: (batch_size, k)\n\n    # Optionally apply the variance-based prior\n    # This is like ridge regularization in a Bayesian sense,\n    # assuming white noise on the inputs: X_proj / (1 + 1 / eigenvalues).\n    if self.use_prior:\n        if self.eigenvalues is None:\n            raise ValueError(\n                \"Eigenvalues not available. PCA must be computed first.\"\n            )\n        X_proj = X_proj / (1.0 + (1.0 / self.eigenvalues)).unsqueeze(0)\n\n    # Reconstruct from the principal components\n    X_reconstructed = X_proj @ self.components\n\n    # Optionally normalize the output so that the average variance is ~1\n    if self.normalize_output:\n        if self.eigenvalues is None:\n            raise ValueError(\n                \"Eigenvalues not available. PCA must be computed first.\"\n            )\n        # The sum of eigenvalues is the total variance in the top-k subspace\n        # We divide by sqrt( average variance per feature ) = sqrt( sum / D )\n        input_dim = X_reconstructed.shape[-1]\n        scale_factor = (self.eigenvalues.sum() / input_dim) ** 0.5\n        X_reconstructed /= scale_factor\n\n    return X_reconstructed\n</code></pre>"},{"location":"api/flow-density/","title":"FlowDensity","text":"<p>Normalizing flow networks for density estimation.</p>"},{"location":"api/flow-density/#overview","title":"Overview","text":"<p><code>FlowDensity</code> is the internal <code>nn.Module</code> that wraps various normalizing flow architectures for use in posterior estimation. It is used internally by the Flow estimator.</p>"},{"location":"api/flow-density/#supported-flow-types","title":"Supported Flow Types","text":"Type Library Description <code>nsf</code> sbi/nflows Neural Spline Flow (recommended) <code>maf</code> sbi/nflows Masked Autoregressive Flow <code>made</code> sbi Masked Autoencoder for Distribution Estimation <code>maf_rqs</code> sbi MAF with Rational Quadratic Splines <code>zuko_nice</code> Zuko NICE architecture <code>zuko_maf</code> Zuko Masked Autoregressive Flow (Zuko) <code>zuko_nsf</code> Zuko Neural Spline Flow (Zuko) <code>zuko_ncsf</code> Zuko Neural Circular Spline Flow <code>zuko_sospf</code> Zuko Sum-of-Squares Polynomial Flow <code>zuko_naf</code> Zuko Neural Autoregressive Flow <code>zuko_unaf</code> Zuko Unconstrained NAF <code>zuko_gf</code> Zuko Glow Flow <code>zuko_bpf</code> Zuko Bernstein Polynomial Flow"},{"location":"api/flow-density/#class-reference","title":"Class Reference","text":""},{"location":"api/flow-density/#falcon.estimators.flow_density.FlowDensity","title":"FlowDensity","text":"<pre><code>FlowDensity(theta, s, theta_norm=False, norm_momentum=0.003, net_type='nsf', use_log_update=False, adaptive_momentum=False)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Normalizing flow network with optional parameter normalization.</p> Source code in <code>falcon/estimators/flow_density.py</code> <pre><code>def __init__(\n    self,\n    theta,\n    s,\n    theta_norm=False,\n    norm_momentum=3e-3,\n    net_type=\"nsf\",\n    use_log_update=False,\n    adaptive_momentum=False,\n):\n    super().__init__()\n    self.theta_norm = (\n        LazyOnlineNorm(\n            momentum=norm_momentum,\n            use_log_update=use_log_update,\n            adaptive_momentum=adaptive_momentum,\n        )\n        if theta_norm\n        else None\n    )\n\n    net_builders = _get_net_builders()\n    builder = net_builders.get(net_type)\n    if builder is None:\n        raise ValueError(f\"Unknown net_type: {net_type}. Available: {list(net_builders.keys())}\")\n    self.net = builder(theta.float(), s.float(), z_score_x=None, z_score_y=None)\n\n    if self.theta_norm is not None:\n        self.theta_norm(theta)  # Initialize normalization stats\n    self.scale = 0.2\n</code></pre>"},{"location":"api/flow-density/#falcon.estimators.flow_density.FlowDensity.loss","title":"loss","text":"<pre><code>loss(theta, s)\n</code></pre> <p>Compute negative log-likelihood loss.</p> Source code in <code>falcon/estimators/flow_density.py</code> <pre><code>def loss(self, theta, s):\n    \"\"\"Compute negative log-likelihood loss.\"\"\"\n    if self.theta_norm is not None:\n        theta = self.theta_norm(theta)\n    theta = theta.float() * self.scale\n    loss = self.net.loss(theta, condition=s.float())\n    loss = loss - np.log(self.scale) * theta.shape[-1]\n    if self.theta_norm is not None:\n        loss = loss + torch.log(self.theta_norm.volume())\n    return loss\n</code></pre>"},{"location":"api/flow-density/#falcon.estimators.flow_density.FlowDensity.sample","title":"sample","text":"<pre><code>sample(num_samples, s)\n</code></pre> <p>Sample from the flow.</p> Source code in <code>falcon/estimators/flow_density.py</code> <pre><code>def sample(self, num_samples, s):\n    \"\"\"Sample from the flow.\"\"\"\n    samples = self.net.sample((num_samples,), condition=s).detach()\n    samples = samples / self.scale\n    if self.theta_norm is not None:\n        samples = self.theta_norm.inverse(samples).detach()\n    return samples\n</code></pre>"},{"location":"api/flow-density/#falcon.estimators.flow_density.FlowDensity.log_prob","title":"log_prob","text":"<pre><code>log_prob(theta, s)\n</code></pre> <p>Compute log probability.</p> Source code in <code>falcon/estimators/flow_density.py</code> <pre><code>def log_prob(self, theta, s):\n    \"\"\"Compute log probability.\"\"\"\n    if self.theta_norm is not None:\n        theta = self.theta_norm(theta).detach()\n    theta = theta * self.scale\n    log_prob = self.net.log_prob(theta.float(), condition=s.float())\n    log_prob = log_prob + np.log(self.scale) * theta.shape[-1]\n    if self.theta_norm is not None:\n        log_prob = log_prob - torch.log(self.theta_norm.volume().detach())\n    return log_prob\n</code></pre>"},{"location":"api/flow/","title":"Flow Estimator","text":"<p>Flow-based posterior estimation using normalizing flows.</p>"},{"location":"api/flow/#overview","title":"Overview","text":"<p><code>Flow</code> is the primary estimator in Falcon for learning posterior distributions. It uses dual normalizing flows (conditional and marginal) with importance sampling for adaptive proposal generation.</p> <p>Key features:</p> <ul> <li>Dual flow architecture for posterior and proposal sampling</li> <li>Parameter space normalization via hypercube mapping</li> <li>Importance sampling with effective sample size monitoring</li> <li>Automatic learning rate scheduling and early stopping</li> </ul>"},{"location":"api/flow/#configuration","title":"Configuration","text":"<p>Flow is configured through four groups: <code>loop</code>, <code>network</code>, <code>optimizer</code>, and <code>inference</code>. The <code>embedding</code> configuration sits as a sibling of <code>network</code>, not nested inside it.</p> <pre><code>estimator:\n  _target_: falcon.estimators.Flow\n\n  loop:\n    # Training loop parameters\n\n  network:\n    # Neural network architecture\n\n  embedding:\n    # Observation embedding network\n\n  optimizer:\n    # Learning rate and scheduling\n\n  inference:\n    # Sampling and amortization settings\n</code></pre>"},{"location":"api/flow/#configuration-reference","title":"Configuration Reference","text":""},{"location":"api/flow/#training-loop-loop","title":"Training Loop (<code>loop</code>)","text":"<p>Controls the training process.</p> Parameter Type Default Description <code>num_epochs</code> int 300 Maximum training epochs <code>batch_size</code> int 128 Training batch size <code>early_stop_patience</code> int 32 Epochs without improvement before stopping <code>reset_network_after_pause</code> bool false Reset network weights when training resumes after pause <code>cache_sync_every</code> int 0 Epochs between cache syncs with the buffer (0 = every epoch) <code>max_cache_samples</code> int 0 Maximum samples to cache (0 = cache all available) <code>cache_on_device</code> bool false Keep cached training data on the estimator's device (e.g. GPU) <pre><code>loop:\n  num_epochs: 300\n  batch_size: 128\n  early_stop_patience: 32\n  reset_network_after_pause: false\n  cache_sync_every: 0\n  max_cache_samples: 0\n  cache_on_device: false\n</code></pre>"},{"location":"api/flow/#data-caching","title":"Data Caching","text":"<p>Training data is loaded into a local cache that is periodically synced with the shared simulation buffer. This avoids repeated remote data fetches and allows fast random-access batching.</p> <ul> <li><code>cache_sync_every</code>: Controls how often the cache pulls new samples from the buffer. A value of <code>0</code> (default) syncs every epoch. Higher values reduce sync overhead at the cost of slightly stale data, which can be useful when simulations are slow.</li> <li><code>max_cache_samples</code>: Caps the number of samples held in the cache. Set to <code>0</code> to cache everything. A positive value randomly subsamples, which helps limit GPU memory usage for very large buffers.</li> <li><code>cache_on_device</code>: When <code>true</code>, cached tensors are moved to the estimator's device (typically GPU) once during sync rather than per-batch. This eliminates CPU-to-GPU transfer overhead during training but increases device memory usage.</li> </ul>"},{"location":"api/flow/#network-architecture-network","title":"Network Architecture (<code>network</code>)","text":"<p>Defines the neural network structure.</p> Parameter Type Default Description <code>net_type</code> str <code>nsf</code> Flow architecture (see FlowDensity for all types) <code>theta_norm</code> bool true Normalize parameter space <code>norm_momentum</code> float 0.003 Momentum for online normalization updates <code>use_log_update</code> bool false Use log-space variance updates <code>adaptive_momentum</code> bool false Sample-dependent momentum <pre><code>network:\n  net_type: nsf\n  theta_norm: true\n  norm_momentum: 0.003\n  use_log_update: false\n  adaptive_momentum: false\n</code></pre>"},{"location":"api/flow/#embedding","title":"Embedding","text":"<p>The embedding network processes observations before they enter the flow. It is configured as a sibling of <code>network</code> (not nested inside it).</p> <pre><code>embedding:\n  _target_: model.MyEmbedding\n  _input_: [x]\n</code></pre> <p>See Embeddings for details on the declarative embedding system, including multi-input and nested pipeline configurations.</p>"},{"location":"api/flow/#optimizer-optimizer","title":"Optimizer (<code>optimizer</code>)","text":"<p>Controls learning rate and scheduling.</p> Parameter Type Default Description <code>lr</code> float 0.01 Initial learning rate <code>lr_decay_factor</code> float 0.5 LR multiplier when plateau detected <code>scheduler_patience</code> int 16 Epochs without improvement before LR decay <pre><code>optimizer:\n  lr: 0.01\n  lr_decay_factor: 0.5\n  scheduler_patience: 16\n</code></pre>"},{"location":"api/flow/#inference-inference","title":"Inference (<code>inference</code>)","text":"<p>Controls posterior sampling and amortization.</p> Parameter Type Default Description <code>gamma</code> float 0.5 Amortization mixing coefficient (0=focused, 1=amortized) <code>discard_samples</code> bool false Discard low-likelihood samples during training <code>log_ratio_threshold</code> float -20 Log-likelihood threshold for sample discarding <code>sample_reference_posterior</code> bool false Sample from reference posterior <code>use_best_models_during_inference</code> bool true Use best validation model for sampling <pre><code>inference:\n  gamma: 0.5\n  discard_samples: false\n  log_ratio_threshold: -20\n  sample_reference_posterior: false\n  use_best_models_during_inference: true\n</code></pre>"},{"location":"api/flow/#understanding-gamma-amortization","title":"Understanding <code>gamma</code> (Amortization)","text":"<p>The <code>gamma</code> parameter controls the trade-off between focused and amortized inference:</p> <ul> <li><code>gamma=0</code>: Fully focused on the specific observation (best for single-observation inference)</li> <li><code>gamma=1</code>: Fully amortized (network generalizes across observations)</li> <li><code>gamma=0.5</code>: Balanced (default, good for most cases)</li> </ul>"},{"location":"api/flow/#embedding-networks","title":"Embedding Networks","text":"<p>Flow requires an embedding network to process observations. The embedding maps high-dimensional observations to a lower-dimensional summary statistic.</p>"},{"location":"api/flow/#basic-embedding","title":"Basic Embedding","text":"<pre><code>embedding:\n  _target_: model.MyEmbedding\n  _input_: [x]\n</code></pre>"},{"location":"api/flow/#multi-input-embedding","title":"Multi-Input Embedding","text":"<pre><code>embedding:\n  _target_: model.MyEmbedding\n  _input_: [x, y]  # Multiple observation nodes\n</code></pre>"},{"location":"api/flow/#nested-embedding-pipeline","title":"Nested Embedding Pipeline","text":"<pre><code>embedding:\n  _target_: model.Concatenate\n  _input_:\n    - _target_: timm.create_model\n      model_name: resnet18\n      pretrained: true\n      num_classes: 0\n      _input_:\n        _target_: model.Unsqueeze\n        _input_: [image]\n    - _target_: torch.nn.Linear\n      in_features: 64\n      out_features: 32\n      _input_: [metadata]\n</code></pre>"},{"location":"api/flow/#complete-example","title":"Complete Example","text":"<pre><code>graph:\n  z:\n    evidence: [x]\n\n    simulator:\n      _target_: falcon.priors.Hypercube\n      priors:\n        - ['uniform', -100.0, 100.0]\n        - ['uniform', -100.0, 100.0]\n        - ['uniform', -100.0, 100.0]\n\n    estimator:\n      _target_: falcon.estimators.Flow\n\n      loop:\n        num_epochs: 300\n        batch_size: 128\n        early_stop_patience: 32\n        cache_sync_every: 0\n        max_cache_samples: 0\n\n      network:\n        net_type: nsf\n        theta_norm: true\n        norm_momentum: 0.003\n\n      embedding:\n        _target_: model.E\n        _input_: [x]\n\n      optimizer:\n        lr: 0.01\n        lr_decay_factor: 0.5\n        scheduler_patience: 16\n\n      inference:\n        gamma: 0.5\n        discard_samples: false\n        log_ratio_threshold: -20\n\n    ray:\n      num_gpus: 0\n\n  x:\n    parents: [z]\n    simulator:\n      _target_: model.Simulate\n    observed: \"./data/obs.npz['x']\"\n</code></pre>"},{"location":"api/flow/#training-strategies","title":"Training Strategies","text":""},{"location":"api/flow/#standard-training","title":"Standard Training","text":"<p>Default configuration with continuous resampling:</p> <pre><code>buffer:\n  min_training_samples: 4096\n  max_training_samples: 32768\n  resample_batch_size: 128\n  keep_resampling: true\n  resample_interval: 10\n</code></pre>"},{"location":"api/flow/#amortized-training","title":"Amortized Training","text":"<p>Fixed dataset without resampling (for learning across many observations):</p> <pre><code>buffer:\n  min_training_samples: 32000\n  max_training_samples: 32000\n  resample_batch_size: 0       # No resampling\n  keep_resampling: false\n\n# Higher gamma for amortization\ninference:\n  gamma: 0.8\n</code></pre>"},{"location":"api/flow/#round-based-training","title":"Round-Based Training","text":"<p>Large batch renewal for sequential refinement:</p> <pre><code>buffer:\n  min_training_samples: 8000\n  max_training_samples: 8000\n  resample_batch_size: 8000    # Full renewal\n  keep_resampling: true\n  resample_interval: 30        # Less frequent\n\ninference:\n  discard_samples: true        # Remove poor samples\n</code></pre>"},{"location":"api/flow/#logged-metrics","title":"Logged Metrics","text":"<p>Flow logs the following metrics during training:</p> Metric Description <code>loss/train</code> Training loss (negative log-likelihood) <code>loss/val</code> Validation loss <code>lr</code> Current learning rate <code>epoch</code> Training epoch <code>best_val_loss</code> Best validation loss seen"},{"location":"api/flow/#tips","title":"Tips","text":"<ol> <li>Start with defaults: The default configuration works well for most problems</li> <li>Increase <code>num_epochs</code> for complex posteriors</li> <li>Enable <code>discard_samples</code> if training becomes unstable with outliers</li> <li>Use GPU (<code>ray.num_gpus: 1</code>) for faster training with large embeddings</li> <li>Lower <code>gamma</code> for single-observation inference, higher for amortization</li> <li>Adjust <code>early_stop_patience</code> based on expected convergence time</li> <li>Set <code>cache_on_device: true</code> when GPU memory permits, to eliminate per-batch CPU-to-GPU transfers</li> <li>Increase <code>cache_sync_every</code> (e.g. 5-10) when simulations are slow and training data changes infrequently</li> </ol>"},{"location":"api/flow/#class-reference","title":"Class Reference","text":""},{"location":"api/flow/#falcon.estimators.flow.Flow","title":"Flow","text":"<pre><code>Flow(simulator_instance, theta_key=None, condition_keys=None, config=None)\n</code></pre> <p>               Bases: <code>StepwiseEstimator</code></p> <p>Flow-based posterior estimation (formerly SNPE_A).</p> <p>Implementation-specific features: - Dual flow architecture (conditional + marginal) - Parameter space normalization via hypercube mapping - Importance sampling for posterior/proposal</p> <p>Initialize Flow estimator.</p> <p>Parameters:</p> Name Type Description Default <code>simulator_instance</code> <p>Prior/simulator instance</p> required <code>theta_key</code> <code>Optional[str]</code> <p>Key for theta in batch data</p> <code>None</code> <code>condition_keys</code> <code>Optional[List[str]]</code> <p>Keys for condition data in batch</p> <code>None</code> <code>config</code> <code>Optional[dict]</code> <p>Configuration dict with loop, network, optimizer, inference sections</p> <code>None</code> Source code in <code>falcon/estimators/flow.py</code> <pre><code>def __init__(\n    self,\n    simulator_instance,\n    theta_key: Optional[str] = None,\n    condition_keys: Optional[List[str]] = None,\n    config: Optional[dict] = None,\n):\n    \"\"\"\n    Initialize Flow estimator.\n\n    Args:\n        simulator_instance: Prior/simulator instance\n        theta_key: Key for theta in batch data\n        condition_keys: Keys for condition data in batch\n        config: Configuration dict with loop, network, optimizer, inference sections\n    \"\"\"\n    # Merge user config with defaults using OmegaConf structured config\n    schema = OmegaConf.structured(FlowConfig)\n    config = OmegaConf.merge(schema, config or {})\n\n    super().__init__(\n        simulator_instance=simulator_instance,\n        loop_config=config.loop,\n        theta_key=theta_key,\n        condition_keys=condition_keys,\n    )\n\n    self.config = config\n\n    # Device setup\n    self.device = self._setup_device(config.device)\n\n    # Embedding network\n    embedding_config = OmegaConf.to_container(config.embedding, resolve=True)\n    self._embedding = instantiate_embedding(embedding_config).to(self.device)\n\n    # Flow networks (initialized lazily)\n    self._conditional_flow = None\n    self._marginal_flow = None\n    self._best_conditional_flow = None\n    self._best_marginal_flow = None\n    self._best_embedding = None\n    self._init_parameters = None\n\n    # Best loss tracking\n    self.best_conditional_flow_val_loss = float(\"inf\")\n    self.best_marginal_flow_val_loss = float(\"inf\")\n\n    # Optimizer/scheduler (initialized lazily)\n    self._optimizer = None\n    self._scheduler = None\n\n    # Extended history for Flow-specific tracking\n    self.history.update({\n        \"theta_mins\": [],\n        \"theta_maxs\": [],\n    })\n</code></pre>"},{"location":"api/flow/#falcon.estimators.flow.Flow.train_step","title":"train_step","text":"<pre><code>train_step(batch)\n</code></pre> <p>Flow training step with gradient update and optional sample discarding.</p> Source code in <code>falcon/estimators/flow.py</code> <pre><code>def train_step(self, batch) -&gt; Dict[str, float]:\n    \"\"\"Flow training step with gradient update and optional sample discarding.\"\"\"\n    ids, theta, theta_logprob, conditions, u, u_device, conditions_device = \\\n        self._unpack_batch(batch, \"train\")\n\n    # Initialize networks on first batch\n    if not self.networks_initialized:\n        self._initialize_networks(u, conditions)\n\n    # Embed conditions\n    s = self._embed(conditions_device, train=True)\n\n    # Track theta ranges\n    with torch.no_grad():\n        self.history[\"theta_mins\"].append(theta.min(dim=0).values.cpu().numpy())\n        self.history[\"theta_maxs\"].append(theta.max(dim=0).values.cpu().numpy())\n\n    # Forward and backward pass\n    self._optimizer.zero_grad()\n    loss_cond, loss_marg = self._compute_flow_losses(u_device, s, train=True)\n    (loss_cond + loss_marg).backward()\n    self._optimizer.step()\n\n    # Discard samples based on log-likelihood ratio\n    if self.config.inference.discard_samples:\n        discard_mask = self._compute_discard_mask(theta, theta_logprob, conditions_device)\n        batch.discard(discard_mask)\n\n    return {\"loss\": loss_cond.item(), \"loss_aux\": loss_marg.item()}\n</code></pre>"},{"location":"api/flow/#falcon.estimators.flow.Flow.val_step","title":"val_step","text":"<pre><code>val_step(batch)\n</code></pre> <p>Flow validation step without gradient computation.</p> Source code in <code>falcon/estimators/flow.py</code> <pre><code>def val_step(self, batch) -&gt; Dict[str, float]:\n    \"\"\"Flow validation step without gradient computation.\"\"\"\n    _, theta, theta_logprob, conditions, u, u_device, conditions_device = \\\n        self._unpack_batch(batch, \"val\")\n\n    # Embed conditions (eval mode)\n    s = self._embed(conditions_device, train=False)\n\n    # Compute losses without gradients\n    with torch.no_grad():\n        loss_cond, loss_marg = self._compute_flow_losses(u_device, s, train=False)\n\n    return {\"loss\": loss_cond.item(), \"loss_aux\": loss_marg.item()}\n</code></pre>"},{"location":"api/flow/#falcon.estimators.flow.Flow.sample_prior","title":"sample_prior","text":"<pre><code>sample_prior(num_samples, conditions=None)\n</code></pre> <p>Sample from the prior distribution.</p> Source code in <code>falcon/estimators/flow.py</code> <pre><code>def sample_prior(self, num_samples: int, conditions: Optional[Dict] = None) -&gt; dict:\n    \"\"\"Sample from the prior distribution.\"\"\"\n    if conditions:\n        raise ValueError(\"Conditions are not supported for sample_prior.\")\n    samples = self.simulator_instance.simulate_batch(num_samples)\n    # Log probability for uniform prior over hypercube [-bound, bound]^d\n    bound = self.config.inference.hypercube_bound\n    log_prob = np.ones(num_samples) * (-np.log(2 * bound) ** self.param_dim)\n    return {'value': samples, 'log_prob': log_prob}\n</code></pre>"},{"location":"api/flow/#falcon.estimators.flow.Flow.sample_posterior","title":"sample_posterior","text":"<pre><code>sample_posterior(num_samples, conditions=None)\n</code></pre> <p>Sample from the posterior distribution q(theta|x).</p> Source code in <code>falcon/estimators/flow.py</code> <pre><code>def sample_posterior(\n    self,\n    num_samples: int,\n    conditions: Optional[Dict] = None,\n) -&gt; dict:\n    \"\"\"Sample from the posterior distribution q(theta|x).\"\"\"\n    # Fall back to prior if networks not yet initialized (training hasn't started)\n    if not self.networks_initialized:\n        return self.sample_prior(num_samples)\n\n    samples, logprob = self._importance_sample(num_samples, mode=\"posterior\", conditions=conditions or {})\n    return {'value': samples.numpy(), 'log_prob': logprob.numpy()}\n</code></pre>"},{"location":"api/flow/#falcon.estimators.flow.Flow.sample_proposal","title":"sample_proposal","text":"<pre><code>sample_proposal(num_samples, conditions=None)\n</code></pre> <p>Sample from the widened proposal distribution for adaptive resampling.</p> Source code in <code>falcon/estimators/flow.py</code> <pre><code>def sample_proposal(\n    self,\n    num_samples: int,\n    conditions: Optional[Dict] = None,\n) -&gt; dict:\n    \"\"\"Sample from the widened proposal distribution for adaptive resampling.\"\"\"\n    # Fall back to prior if networks not yet initialized (training hasn't started)\n    if not self.networks_initialized:\n        return self.sample_prior(num_samples)\n\n    cfg_inf = self.config.inference\n    conditions = conditions or {}\n\n    if cfg_inf.sample_reference_posterior:\n        post_samples, _ = self._importance_sample(cfg_inf.reference_samples, mode=\"posterior\", conditions=conditions)\n        mean, std = post_samples.mean(dim=0).cpu(), post_samples.std(dim=0).cpu()\n        log({f\"sample_proposal:posterior_mean_{i}\": mean[i].item() for i in range(len(mean))})\n        log({f\"sample_proposal:posterior_std_{i}\": std[i].item() for i in range(len(std))})\n\n    samples, logprob = self._importance_sample(num_samples, mode=\"proposal\", conditions=conditions)\n    log({\n        \"sample_proposal:mean\": samples.mean().item(),\n        \"sample_proposal:std\": samples.std().item(),\n        \"sample_proposal:logprob\": logprob.mean().item(),\n    })\n    return {'value': samples.numpy(), 'log_prob': logprob.numpy()}\n</code></pre>"},{"location":"api/flow/#falcon.estimators.flow.Flow.save","title":"save","text":"<pre><code>save(node_dir)\n</code></pre> <p>Save Flow state.</p> Source code in <code>falcon/estimators/flow.py</code> <pre><code>def save(self, node_dir: Path) -&gt; None:\n    \"\"\"Save Flow state.\"\"\"\n    debug(f\"Saving: {node_dir}\")\n    if not self.networks_initialized:\n        raise RuntimeError(\"Networks not initialized.\")\n\n    torch.save(self._best_conditional_flow.state_dict(), node_dir / \"conditional_flow.pth\")\n    torch.save(self._best_marginal_flow.state_dict(), node_dir / \"marginal_flow.pth\")\n    torch.save(self._init_parameters, node_dir / \"init_parameters.pth\")\n\n    # Save history\n    torch.save(self.history[\"train_ids\"], node_dir / \"train_id_history.pth\")\n    torch.save(self.history[\"val_ids\"], node_dir / \"validation_id_history.pth\")\n    torch.save(self.history[\"theta_mins\"], node_dir / \"theta_mins_batches.pth\")\n    torch.save(self.history[\"theta_maxs\"], node_dir / \"theta_maxs_batches.pth\")\n    torch.save(self.history[\"epochs\"], node_dir / \"epochs.pth\")\n    torch.save(self.history[\"train_loss\"], node_dir / \"loss_train_posterior.pth\")\n    torch.save(self.history[\"val_loss\"], node_dir / \"loss_val_posterior.pth\")\n    torch.save(self.history[\"n_samples\"], node_dir / \"n_samples_total.pth\")\n    torch.save(self.history[\"elapsed_min\"], node_dir / \"elapsed_minutes.pth\")\n\n    if self._best_embedding is not None:\n        torch.save(self._best_embedding.state_dict(), node_dir / \"embedding.pth\")\n</code></pre>"},{"location":"api/flow/#falcon.estimators.flow.Flow.load","title":"load","text":"<pre><code>load(node_dir)\n</code></pre> <p>Load Flow state.</p> Source code in <code>falcon/estimators/flow.py</code> <pre><code>def load(self, node_dir: Path) -&gt; None:\n    \"\"\"Load Flow state.\"\"\"\n    debug(f\"Loading: {node_dir}\")\n    init_parameters = torch.load(node_dir / \"init_parameters.pth\")\n    self._initialize_networks(init_parameters[0], init_parameters[1])\n\n    self._best_conditional_flow.load_state_dict(\n        torch.load(node_dir / \"conditional_flow.pth\")\n    )\n    self._best_marginal_flow.load_state_dict(\n        torch.load(node_dir / \"marginal_flow.pth\")\n    )\n\n    if (node_dir / \"embedding.pth\").exists() and self._best_embedding is not None:\n        self._best_embedding.load_state_dict(torch.load(node_dir / \"embedding.pth\"))\n</code></pre>"},{"location":"api/flow/#configuration-classes","title":"Configuration Classes","text":""},{"location":"api/flow/#falcon.estimators.flow.FlowConfig","title":"FlowConfig  <code>dataclass</code>","text":"<pre><code>FlowConfig(loop=TrainingLoopConfig(), network=NetworkConfig(), optimizer=OptimizerConfig(), inference=InferenceConfig(), embedding=None, device=None)\n</code></pre> <p>Top-level Flow estimator configuration.</p>"},{"location":"api/flow/#falcon.estimators.flow.NetworkConfig","title":"NetworkConfig  <code>dataclass</code>","text":"<pre><code>NetworkConfig(net_type='zuko_nice', theta_norm=True, norm_momentum=0.01, adaptive_momentum=False, use_log_update=False)\n</code></pre> <p>Neural network architecture parameters.</p>"},{"location":"api/flow/#falcon.estimators.flow.OptimizerConfig","title":"OptimizerConfig  <code>dataclass</code>","text":"<pre><code>OptimizerConfig(lr=0.01, lr_decay_factor=0.1, scheduler_patience=8)\n</code></pre> <p>Optimizer parameters (training-time).</p>"},{"location":"api/flow/#falcon.estimators.flow.InferenceConfig","title":"InferenceConfig  <code>dataclass</code>","text":"<pre><code>InferenceConfig(gamma=0.5, discard_samples=True, log_ratio_threshold=-20.0, sample_reference_posterior=False, use_best_models_during_inference=True, num_proposals=256, reference_samples=128, hypercube_bound=2.0, out_of_bounds_penalty=100.0, nan_replacement=-100.0)\n</code></pre> <p>Inference and sampling parameters.</p>"},{"location":"api/flow/#falcon.estimators.base.TrainingLoopConfig","title":"TrainingLoopConfig  <code>dataclass</code>","text":"<pre><code>TrainingLoopConfig(num_epochs=100, batch_size=128, early_stop_patience=16, reset_network_after_pause=False, cache_sync_every=0, max_cache_samples=0, cache_on_device=False)\n</code></pre> <p>Generic training loop parameters.</p>"},{"location":"api/gaussian/","title":"Gaussian Estimator","text":"<p>Full covariance Gaussian posterior estimation.</p>"},{"location":"api/gaussian/#overview","title":"Overview","text":"<p><code>Gaussian</code> provides a simpler alternative to Flow for posterior estimation. Instead of normalizing flows, it models the posterior as a multivariate Gaussian with full covariance, using eigenvalue-based operations.</p> <p>Key features:</p> <ul> <li>Full covariance matrix showing parameter correlations directly</li> <li>Eigenvalue-based tempered sampling for exploration</li> <li>Simpler and more interpretable than flow-based methods</li> </ul> <p>Note</p> <p><code>Gaussian</code> requires a <code>Product</code> prior (with <code>\"standard_normal\"</code> mode) as the simulator, not <code>Hypercube</code>.</p>"},{"location":"api/gaussian/#configuration","title":"Configuration","text":"<p>Gaussian is configured through the same four groups as Flow: <code>loop</code>, <code>network</code>, <code>optimizer</code>, and <code>inference</code>.</p> <pre><code>estimator:\n  _target_: falcon.estimators.Gaussian\n  loop:\n    num_epochs: 1000\n    batch_size: 128\n    early_stop_patience: 32\n  network:\n    hidden_dim: 128\n    num_layers: 3\n    momentum: 0.10\n    min_var: 1.0e-20\n    eig_update_freq: 1\n  embedding:\n    _target_: model.E_identity\n    _input_: [x]\n  optimizer:\n    lr: 0.01\n    lr_decay_factor: 0.5\n    scheduler_patience: 16\n  inference:\n    gamma: 1.0\n    discard_samples: false\n    log_ratio_threshold: -20.0\n</code></pre>"},{"location":"api/gaussian/#configuration-reference","title":"Configuration Reference","text":""},{"location":"api/gaussian/#network-network","title":"Network (<code>network</code>)","text":"Parameter Type Default Description <code>hidden_dim</code> int 128 MLP hidden layer dimension <code>num_layers</code> int 3 Number of hidden layers <code>momentum</code> float 0.10 EMA momentum for running statistics <code>min_var</code> float 1e-20 Minimum variance for numerical stability <code>eig_update_freq</code> int 1 Eigendecomposition update frequency <p>The <code>loop</code>, <code>optimizer</code>, and <code>inference</code> groups share the same parameters as Flow.</p>"},{"location":"api/gaussian/#complete-example","title":"Complete Example","text":"<pre><code>graph:\n  z:\n    evidence: [x]\n\n    simulator:\n      _target_: falcon.priors.Product\n      priors:\n        - ['normal', 0.0, 1.0]\n        - ['normal', 0.0, 1.0]\n        - ['normal', 0.0, 1.0]\n\n    estimator:\n      _target_: falcon.estimators.Gaussian\n      loop:\n        num_epochs: 1000\n        batch_size: 128\n        early_stop_patience: 32\n      network:\n        hidden_dim: 128\n        num_layers: 3\n        momentum: 0.10\n        min_var: 1.0e-20\n        eig_update_freq: 1\n      embedding:\n        _target_: model.E_identity\n        _input_: [x]\n      optimizer:\n        lr: 0.01\n        lr_decay_factor: 0.5\n        scheduler_patience: 16\n      inference:\n        gamma: 1.0\n        discard_samples: false\n        log_ratio_threshold: -20.0\n\n    ray:\n      num_gpus: 1\n\n  x:\n    parents: [z]\n    simulator:\n      _target_: model.ExpPlusNoise\n      sigma: 1.0e-6\n    observed: \"./data/mock_data.npz['x']\"\n\nsample:\n  posterior:\n    n: 1000\n</code></pre>"},{"location":"api/gaussian/#class-reference","title":"Class Reference","text":""},{"location":"api/gaussian/#falcon.estimators.gaussian.Gaussian","title":"Gaussian","text":"<pre><code>Gaussian(simulator_instance, theta_key=None, condition_keys=None, config=None)\n</code></pre> <p>Create a LossBasedEstimator with GaussianPosterior.</p> <p>This is the main entry point for using Gaussian posterior estimation. It provides sensible defaults while allowing full customization.</p> <p>Parameters:</p> Name Type Description Default <code>simulator_instance</code> <p>Prior/simulator instance</p> required <code>theta_key</code> <code>Optional[str]</code> <p>Key for theta in batch data</p> <code>None</code> <code>condition_keys</code> <code>Optional[List[str]]</code> <p>Keys for condition data in batch</p> <code>None</code> <code>config</code> <code>Optional[dict]</code> <p>Configuration dict with sections: - loop: TrainingLoopConfig options - network: GaussianPosteriorConfig options - optimizer: OptimizerConfig options - inference: InferenceConfig options - embedding: Embedding configuration with target - device: Device string (optional)</p> <code>None</code> <p>Returns:</p> Type Description <code>LossBasedEstimator</code> <p>Configured LossBasedEstimator ready for training</p> Example YAML <p>estimator:   target: falcon.estimators.Gaussian   network:     hidden_dim: 128     num_layers: 3   embedding:     target: model.E     input: [x]</p> Source code in <code>falcon/estimators/gaussian.py</code> <pre><code>def Gaussian(\n    simulator_instance,\n    theta_key: Optional[str] = None,\n    condition_keys: Optional[List[str]] = None,\n    config: Optional[dict] = None,\n) -&gt; LossBasedEstimator:\n    \"\"\"Create a LossBasedEstimator with GaussianPosterior.\n\n    This is the main entry point for using Gaussian posterior estimation.\n    It provides sensible defaults while allowing full customization.\n\n    Args:\n        simulator_instance: Prior/simulator instance\n        theta_key: Key for theta in batch data\n        condition_keys: Keys for condition data in batch\n        config: Configuration dict with sections:\n            - loop: TrainingLoopConfig options\n            - network: GaussianPosteriorConfig options\n            - optimizer: OptimizerConfig options\n            - inference: InferenceConfig options\n            - embedding: Embedding configuration with _target_\n            - device: Device string (optional)\n\n    Returns:\n        Configured LossBasedEstimator ready for training\n\n    Example YAML:\n        estimator:\n          _target_: falcon.estimators.Gaussian\n          network:\n            hidden_dim: 128\n            num_layers: 3\n          embedding:\n            _target_: model.E\n            _input_: [x]\n    \"\"\"\n    # Check simulator supports transformation interface\n    if not isinstance(simulator_instance, TransformedPrior):\n        raise TypeError(\n            f\"Gaussian requires a TransformedPrior (e.g., Product), \"\n            f\"got {type(simulator_instance).__name__}. \"\n            f\"The simulator must support forward/inverse with mode='standard_normal'.\"\n        )\n\n    # Merge with defaults\n    schema = OmegaConf.structured(GaussianConfig)\n    cfg = OmegaConf.merge(schema, config or {})\n\n    # Extract configs as plain dicts\n    embedding_config = OmegaConf.to_container(cfg.embedding, resolve=True)\n    posterior_config = OmegaConf.to_container(cfg.network, resolve=True)\n\n    return LossBasedEstimator(\n        simulator_instance=simulator_instance,\n        posterior_cls=GaussianPosterior,\n        embedding_config=embedding_config,\n        loop_config=cfg.loop,\n        optimizer_config=cfg.optimizer,\n        inference_config=cfg.inference,\n        posterior_config=posterior_config,\n        theta_key=theta_key,\n        condition_keys=condition_keys,\n        device=cfg.device,\n        latent_mode=\"standard_normal\",  # GaussianPosterior assumes N(0,I) prior\n    )\n</code></pre>"},{"location":"api/gaussian/#falcon.estimators.gaussian.GaussianPosterior","title":"GaussianPosterior","text":"<pre><code>GaussianPosterior(param_dim, condition_dim, hidden_dim=128, num_layers=3, momentum=0.01, min_var=1e-06, eig_update_freq=1)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Full covariance Gaussian posterior with eigenvalue-based operations.</p> Implements the Posterior contract <ul> <li>loss(theta, conditions) -&gt; Tensor</li> <li>sample(conditions, gamma=None) -&gt; Tensor</li> <li>log_prob(theta, conditions) -&gt; Tensor</li> </ul> The posterior is parameterized as <p>p(theta | conditions) = N(mu(conditions), Sigma)</p> <p>where mu(conditions) is predicted by an MLP with whitening, and Sigma is the residual covariance matrix estimated from training data.</p> Source code in <code>falcon/estimators/gaussian.py</code> <pre><code>def __init__(\n    self,\n    param_dim: int,\n    condition_dim: int,\n    hidden_dim: int = 128,\n    num_layers: int = 3,\n    momentum: float = 0.01,\n    min_var: float = 1e-6,\n    eig_update_freq: int = 1,\n):\n    super().__init__()\n    self.param_dim = param_dim\n    self.condition_dim = condition_dim\n    self.momentum = momentum\n    self.min_var = min_var\n    self.eig_update_freq = eig_update_freq\n    self._step_counter = 0\n\n    # MLP for mean prediction\n    self.net = build_mlp(condition_dim, hidden_dim, param_dim, num_layers)\n\n    # Input statistics (conditions) - diagonal whitening\n    self.register_buffer(\"_input_mean\", torch.zeros(condition_dim))\n    self.register_buffer(\"_input_std\", torch.ones(condition_dim))\n\n    # Output statistics (theta) - diagonal whitening\n    self.register_buffer(\"_output_mean\", torch.zeros(param_dim))\n    self.register_buffer(\"_output_std\", torch.ones(param_dim))\n\n    # Residual covariance (prediction error) - full covariance\n    self.register_buffer(\"_residual_cov\", torch.eye(param_dim))\n    self.register_buffer(\"_residual_eigvals\", torch.ones(param_dim))\n    self.register_buffer(\"_residual_eigvecs\", torch.eye(param_dim))\n</code></pre>"},{"location":"api/gaussian/#falcon.estimators.gaussian.GaussianPosterior.loss","title":"loss","text":"<pre><code>loss(theta, conditions)\n</code></pre> <p>Compute negative log likelihood loss, updating statistics only during training.</p> Source code in <code>falcon/estimators/gaussian.py</code> <pre><code>def loss(self, theta: torch.Tensor, conditions: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute negative log likelihood loss, updating statistics only during training.\"\"\"\n    if self.training:\n        self._update_stats(theta, conditions)\n        self._update_residual_cov(theta, conditions)\n    log_prob = self.log_prob(theta, conditions)\n    return -log_prob.mean()\n</code></pre>"},{"location":"api/gaussian/#falcon.estimators.gaussian.GaussianPosterior.log_prob","title":"log_prob","text":"<pre><code>log_prob(theta, conditions)\n</code></pre> <p>Compute Gaussian log probability using eigendecomposition.</p> Source code in <code>falcon/estimators/gaussian.py</code> <pre><code>def log_prob(self, theta: torch.Tensor, conditions: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute Gaussian log probability using eigendecomposition.\"\"\"\n    mean = self._forward_mean(conditions)\n    residuals = theta - mean\n\n    V = self._residual_eigvecs.detach()\n    d = self._residual_eigvals.detach()\n\n    log_det = torch.log(d).sum()\n    r_proj = V.T @ residuals.T\n    mahal = (r_proj**2 / d.unsqueeze(1)).sum(dim=0)\n\n    return -0.5 * (self.param_dim * np.log(2 * np.pi) + log_det + mahal)\n</code></pre>"},{"location":"api/gaussian/#falcon.estimators.gaussian.GaussianPosterior.sample","title":"sample","text":"<pre><code>sample(conditions, gamma=None)\n</code></pre> <p>Sample from posterior, optionally tempered.</p> <p>Parameters:</p> Name Type Description Default <code>conditions</code> <code>Tensor</code> <p>Condition tensor of shape (batch, condition_dim)</p> required <code>gamma</code> <code>Optional[float]</code> <p>Tempering parameter. None = untempered, &lt;1 = widened.</p> <code>None</code> Source code in <code>falcon/estimators/gaussian.py</code> <pre><code>def sample(self, conditions: torch.Tensor, gamma: Optional[float] = None) -&gt; torch.Tensor:\n    \"\"\"Sample from posterior, optionally tempered.\n\n    Args:\n        conditions: Condition tensor of shape (batch, condition_dim)\n        gamma: Tempering parameter. None = untempered, &lt;1 = widened.\n    \"\"\"\n    mean = self._forward_mean(conditions)\n    V = self._residual_eigvecs\n    d = self._residual_eigvals\n\n    if gamma is None:\n        eps = torch.randn_like(mean)\n        return mean + (V @ (torch.sqrt(d).unsqueeze(1) * eps.T)).T\n\n    # Tempered sampling\n    a = gamma / (1 + gamma)\n    lambda_like = (1.0 / d - 1.0).clamp(min=0)\n    lambda_prop = a * lambda_like + 1.0\n    var_prop = 1.0 / lambda_prop\n\n    mean_proj = V.T @ mean.T\n    alpha = a / (d * lambda_prop)\n    mean_prop = (V @ (alpha.unsqueeze(1) * mean_proj)).T\n\n    eps = torch.randn_like(mean)\n    return mean_prop + (V @ (torch.sqrt(var_prop).unsqueeze(1) * eps.T)).T\n</code></pre>"},{"location":"api/gaussian/#configuration-classes","title":"Configuration Classes","text":""},{"location":"api/gaussian/#falcon.estimators.gaussian.GaussianConfig","title":"GaussianConfig  <code>dataclass</code>","text":"<pre><code>GaussianConfig(loop=TrainingLoopConfig(), network=GaussianPosteriorConfig(), optimizer=_default_optimizer_config(), inference=InferenceConfig(), embedding=None, device=None)\n</code></pre> <p>Top-level Gaussian estimator configuration.</p>"},{"location":"api/gaussian/#falcon.estimators.gaussian.GaussianPosteriorConfig","title":"GaussianPosteriorConfig  <code>dataclass</code>","text":"<pre><code>GaussianPosteriorConfig(hidden_dim=128, num_layers=3, momentum=0.01, min_var=1e-20, eig_update_freq=1)\n</code></pre> <p>Configuration for GaussianPosterior network.</p>"},{"location":"api/graph/","title":"Graph","text":"<p>The graph module provides the core data structures for defining probabilistic models.</p>"},{"location":"api/graph/#overview","title":"Overview","text":"<p>A Falcon graph consists of <code>Node</code> objects connected by parent-child relationships. The <code>Graph</code> class manages these relationships and handles topological sorting for correct execution order.</p>"},{"location":"api/graph/#classes","title":"Classes","text":""},{"location":"api/graph/#falcon.core.graph.Node","title":"Node","text":"<pre><code>Node(name, simulator_cls, estimator_cls=None, parents=None, evidence=None, scaffolds=None, observed=False, resample=False, simulator_config=None, estimator_config=None, actor_config=None, num_actors=1)\n</code></pre> <p>Node definition for a graphical model.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the node.</p> required <code>create_distr</code> <code>class</code> <p>Distribution class to create the node.</p> required <code>config</code> <code>dict</code> <p>Configuration for the distribution.</p> required <code>parents</code> <code>list</code> <p>List of parent node names (forward model).</p> <code>None</code> <code>evidence</code> <code>list</code> <p>List of evidence node names (inference model).</p> <code>None</code> <code>observed</code> <code>bool</code> <p>Whether the node is observed (act as root nodes for inference model).</p> <code>False</code> <code>actor_name</code> <code>str</code> <p>Optional name of the actor to deploy the node.</p> required <code>resample</code> <code>bool</code> <p>Whether to resample the node</p> <code>False</code> Source code in <code>falcon/core/graph.py</code> <pre><code>def __init__(\n    self,\n    name,\n    simulator_cls,\n    estimator_cls=None,\n    parents=None,\n    evidence=None,\n    scaffolds=None,\n    observed=False,\n    resample=False,\n    simulator_config=None,\n    estimator_config=None,\n    actor_config=None,\n    num_actors=1,\n):\n    \"\"\"Node definition for a graphical model.\n\n    Args:\n        name (str): Name of the node.\n        create_distr (class): Distribution class to create the node.\n        config (dict): Configuration for the distribution.\n        parents (list): List of parent node names (forward model).\n        evidence (list): List of evidence node names (inference model).\n        observed (bool): Whether the node is observed (act as root nodes for inference model).\n        actor_name (str): Optional name of the actor to deploy the node.\n        resample (bool): Whether to resample the node\n    \"\"\"\n    self.name = name\n\n    self.simulator_cls = simulator_cls\n    self.estimator_cls = estimator_cls\n\n    self.parents = parents or []\n    self.evidence = evidence or []\n    self.scaffolds = scaffolds or []\n    self.observed = observed\n    self.resample = resample\n    self.train = self.estimator_cls is not None\n\n    self.simulator_config = simulator_config or {}\n    self.estimator_config = estimator_config or {}\n    self.actor_config = actor_config or {}\n    self.num_actors = num_actors\n</code></pre>"},{"location":"api/graph/#falcon.core.graph.Graph","title":"Graph","text":"<pre><code>Graph(node_list)\n</code></pre> Source code in <code>falcon/core/graph.py</code> <pre><code>def __init__(self, node_list):\n    # Storing the node list\n    self.node_list = node_list\n    self.node_dict = {node.name: node for node in node_list}\n    self.simulator_cls_dict = {node.name: node.simulator_cls for node in node_list}\n\n    # Storing the model graph structure\n    self.name_list = [node.name for node in node_list]\n    self.parents_dict = {node.name: node.parents for node in node_list}\n    self.sorted_node_names = self._topological_sort(\n        self.name_list, self.parents_dict\n    )\n\n    # Storing the inference graph structure.\n    # Only observed nodes or nodes with evidence are included in the inference graph.\n    self.evidence_dict = {node.name: node.evidence for node in node_list}\n    self.scaffolds_dict = {node.name: node.scaffolds for node in node_list}\n    self.observed_dict = {node.name: node.observed for node in node_list}\n    self.inference_name_list = [\n        node.name for node in node_list if node.observed or len(node.evidence) &gt; 0\n    ]\n    self.sorted_inference_node_names = self._topological_sort(\n        self.inference_name_list, self.evidence_dict\n    )\n</code></pre>"},{"location":"api/graph/#falcon.core.graph.Graph.get_parents","title":"get_parents","text":"<pre><code>get_parents(node_name)\n</code></pre> Source code in <code>falcon/core/graph.py</code> <pre><code>def get_parents(self, node_name):\n    return self.parents_dict[node_name]\n</code></pre>"},{"location":"api/graph/#falcon.core.graph.Graph.get_evidence","title":"get_evidence","text":"<pre><code>get_evidence(node_name)\n</code></pre> Source code in <code>falcon/core/graph.py</code> <pre><code>def get_evidence(self, node_name):\n    return self.evidence_dict[node_name]\n</code></pre>"},{"location":"api/graph/#falcon.core.graph.Graph.get_simulator_cls","title":"get_simulator_cls","text":"<pre><code>get_simulator_cls(node_name)\n</code></pre> Source code in <code>falcon/core/graph.py</code> <pre><code>def get_simulator_cls(self, node_name):\n    return self.simulator_cls_dict[node_name]\n</code></pre>"},{"location":"api/graph/#functions","title":"Functions","text":""},{"location":"api/graph/#falcon.core.graph.CompositeNode","title":"CompositeNode","text":"<pre><code>CompositeNode(names, module, **kwargs)\n</code></pre> <p>Auxiliary function to create a composite node with multiple child nodes.</p> Source code in <code>falcon/core/graph.py</code> <pre><code>def CompositeNode(names, module, **kwargs):\n    \"\"\"Auxiliary function to create a composite node with multiple child nodes.\"\"\"\n\n    # Generate name of composite node from names of child nodes\n    joined_names = \"comp_\" + \"_\".join(names)\n\n    # Instantiate composite node\n    node_comp = Node(joined_names, module, **kwargs)\n\n    # Instantiate child nodes, which extract the individual components\n    nodes = []\n    for i, name in enumerate(names):\n        node = Node(\n            name, Extractor, parents=[joined_names], simulator_config=dict(index=i)\n        )\n        nodes.append(node)\n\n    # Return composite node and child nodes, which both must be added to the graph\n    return node_comp, *nodes\n</code></pre>"},{"location":"api/graph/#falcon.core.graph.create_graph_from_config","title":"create_graph_from_config","text":"<pre><code>create_graph_from_config(graph_config, _cfg=None)\n</code></pre> <p>Create a computational graph from YAML configuration.</p> <p>Parameters:</p> Name Type Description Default <code>graph_config</code> <p>Dictionary containing graph node definitions</p> required <code>_cfg</code> <p>Full Hydra configuration object (optional)</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Graph</code> <p>The computational graph</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration is invalid (missing required fields, unknown references)</p> Source code in <code>falcon/core/graph.py</code> <pre><code>def create_graph_from_config(graph_config, _cfg=None):\n    \"\"\"Create a computational graph from YAML configuration.\n\n    Args:\n        graph_config: Dictionary containing graph node definitions\n        _cfg: Full Hydra configuration object (optional)\n\n    Returns:\n        Graph: The computational graph\n\n    Raises:\n        ValueError: If configuration is invalid (missing required fields, unknown references)\n    \"\"\"\n    nodes = []\n    observations = {}\n\n    for node_name, node_config in graph_config.items():\n        # Validate configuration\n        _validate_node_config(node_name, node_config)\n\n        # Extract node parameters\n        parents = node_config.get(\"parents\", [])\n        evidence = node_config.get(\"evidence\", [])\n        scaffolds = node_config.get(\"scaffolds\", [])\n        observed = node_config.get(\n            \"observed\", False\n        )  # TODO: Remove from internal logic\n        data_path = node_config.get(\"observed\", None)\n        resample = node_config.get(\"resample\", False)\n        actor_config = node_config.get(\"ray\", {})\n        num_actors = node_config.get(\"num_actors\", 1)\n\n        if actor_config != {}:\n            actor_config = OmegaConf.to_container(actor_config, resolve=True)\n\n        if data_path is not None:\n            # Parse path for NPZ key extraction syntax: \"file.npz['key']\"\n            file_path, key = _parse_observation_path(data_path)\n            if not os.path.exists(file_path):\n                raise FileNotFoundError(f\"Observation file not found: {file_path}\")\n            data = np.load(file_path)\n            if key is not None:\n                # Extract specific key from NPZ\n                data = data[key]\n            elif hasattr(data, 'files') and len(data.files) == 1:\n                # Auto-extract single-key NPZ files\n                data = data[data.files[0]]\n            observations[node_name] = data\n\n        # Extract target from simulator\n        simulator = node_config.get(\"simulator\")\n        if isinstance(simulator, str):\n            simulator_cls = simulator\n            simulator_config = {}\n        else:\n            simulator_cls = simulator.get(\"_target_\")\n            simulator_config = simulator\n            simulator_config = OmegaConf.to_container(simulator_config, resolve=True)\n            simulator_config.pop(\"_target_\", None)\n\n        # Extract target from infer\n        if \"estimator\" in node_config:\n            estimator = node_config.get(\"estimator\")\n            if isinstance(estimator, str):\n                estimator_cls = estimator\n                estimator_config = {}\n            else:\n                estimator_cls = estimator.get(\"_target_\")\n                estimator_config = estimator\n                estimator_config = OmegaConf.to_container(\n                    estimator_config, resolve=True\n                )\n                estimator_config.pop(\"_target_\", None)\n        else:\n            estimator_cls = None\n            estimator_config = {}\n\n        # Create the node\n        node = Node(\n            name=node_name,\n            simulator_cls=simulator_cls,\n            estimator_cls=estimator_cls,\n            parents=parents,\n            evidence=evidence,\n            scaffolds=scaffolds,\n            observed=observed,\n            resample=resample,\n            simulator_config=simulator_config,\n            estimator_config=estimator_config,\n            actor_config=actor_config,\n            num_actors=num_actors,\n        )\n\n        nodes.append(node)\n\n    # Validate node references\n    node_names = {node.name for node in nodes}\n    _validate_node_references(nodes, node_names)\n\n    # Create and return the graph\n    return Graph(nodes), observations\n</code></pre>"},{"location":"api/hypercube/","title":"Hypercube","text":"<p>Flexible prior distributions with hypercube mapping.</p>"},{"location":"api/hypercube/#overview","title":"Overview","text":"<p><code>Hypercube</code> maps between a hypercube domain and various target distributions. This enables uniform treatment of different prior types during training while preserving the original distribution semantics.</p> <p>Use <code>Hypercube</code> as the simulator (prior) for latent nodes when pairing with the Flow estimator.</p>"},{"location":"api/hypercube/#supported-distributions","title":"Supported Distributions","text":"Type Parameters Description <code>uniform</code> <code>low</code>, <code>high</code> Uniform distribution <code>normal</code> <code>mean</code>, <code>std</code> Gaussian distribution <code>cosine</code> <code>low</code>, <code>high</code> Distribution with pdf proportional to sin(angle) <code>sine</code> <code>low</code>, <code>high</code> Inverse sine mapping <code>uvol</code> <code>low</code>, <code>high</code> Uniform-in-volume <code>triangular</code> <code>a</code>, <code>c</code>, <code>b</code> Triangular distribution (min, mode, max)"},{"location":"api/hypercube/#usage","title":"Usage","text":"<pre><code>from falcon.priors import Hypercube\n\n# Define priors for 3 parameters\nprior = Hypercube(\n    priors=[\n        ('uniform', -10.0, 10.0),\n        ('normal', 0.0, 1.0),\n        ('triangular', -1.0, 0.0, 1.0),\n    ]\n)\n\n# Sample from prior\nsamples = prior.simulate_batch(1000)\n\n# Transform to/from hypercube space\nu = prior.inverse(samples)   # To hypercube\nx = prior.forward(u)         # From hypercube\n</code></pre>"},{"location":"api/hypercube/#yaml-configuration","title":"YAML Configuration","text":"<pre><code>simulator:\n  _target_: falcon.priors.Hypercube\n  priors:\n    - ['uniform', -10.0, 10.0]\n    - ['normal', 0.0, 1.0]\n  hypercube_range: [-2, 2]\n</code></pre>"},{"location":"api/hypercube/#class-reference","title":"Class Reference","text":""},{"location":"api/hypercube/#falcon.priors.hypercube.Hypercube","title":"Hypercube","text":"<pre><code>Hypercube(priors=[], hypercube_range=[-2, 2])\n</code></pre> <p>Maps a set of univariate priors between a hypercube domain and their target distributions.</p> This class supports a bi-directional transformation <ul> <li>forward: maps from a hypercube domain (default range [-2, 2]) to the target distributions.</li> <li>inverse: maps from the target distribution domain back to the hypercube.</li> </ul> Supported distribution types and their required parameters <ul> <li>\"uniform\": Linear mapping from [0, 1] to [low, high].              Parameters: low, high.</li> <li>\"cosine\": Uses an acos transform for distributions with pdf \u221d sin(angle).             Parameters: low, high.</li> <li>\"sine\": Uses an asin transform for a similar angular mapping.           Parameters: low, high.</li> <li>\"uvol\": Uniform-in-volume transformation.           Parameters: low, high.</li> <li>\"normal\": Maps using the inverse CDF (probit function) for a normal distribution.             Parameters: mean, std.</li> <li>\"triangular\": Maps to a triangular distribution via its inverse CDF.                 Parameters: a (min), c (mode), b (max).</li> </ul> Priors should be provided as a list of tuples <p>(dist_type, param1, param2, ...)</p> <p>For example, a uniform prior would be (\"uniform\", low, high) and a triangular prior would be (\"triangular\", a, c, b).</p> <p>Initializes the Hypercube object.</p> <p>Parameters:</p> Name Type Description Default <code>priors</code> <code>list</code> <p>List of tuples defining each prior. Each tuple starts with a string specifying            the distribution type, followed by its parameters.</p> <code>[]</code> <code>hypercube_range</code> <code>list or tuple</code> <p>The range of the hypercube domain (default: [-2, 2]).</p> <code>[-2, 2]</code> Source code in <code>falcon/priors/hypercube.py</code> <pre><code>def __init__(self, priors=[], hypercube_range=[-2, 2]):\n    \"\"\"\n    Initializes the Hypercube object.\n\n    Args:\n        priors (list): List of tuples defining each prior. Each tuple starts with a string specifying\n                       the distribution type, followed by its parameters.\n        hypercube_range (list or tuple): The range of the hypercube domain (default: [-2, 2]).\n    \"\"\"\n    self.priors = priors\n    self.param_dim = len(priors)\n    self.hypercube_range = hypercube_range\n</code></pre>"},{"location":"api/hypercube/#falcon.priors.hypercube.Hypercube.forward","title":"forward","text":"<pre><code>forward(u)\n</code></pre> <p>Applies the forward transformation to a batch of input values.</p> <p>The input tensor u should have shape (..., n_params), where the last dimension corresponds to different parameters (each in the hypercube_range). First, the values are rescaled to [0,1] and then mapped into the corresponding target distributions.</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <code>Tensor</code> <p>Tensor of shape (..., n_params) with values in the hypercube_range.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Tensor of shape (..., n_params) with values in the target distribution domains.</p> Source code in <code>falcon/priors/hypercube.py</code> <pre><code>def forward(self, u):\n    \"\"\"\n    Applies the forward transformation to a batch of input values.\n\n    The input tensor u should have shape (..., n_params), where the last dimension\n    corresponds to different parameters (each in the hypercube_range). First, the values\n    are rescaled to [0,1] and then mapped into the corresponding target distributions.\n\n    Args:\n        u (torch.Tensor): Tensor of shape (..., n_params) with values in the hypercube_range.\n\n    Returns:\n        torch.Tensor: Tensor of shape (..., n_params) with values in the target distribution domains.\n    \"\"\"\n    # Rescale u from hypercube_range to [0, 1]\n    u = (u - self.hypercube_range[0]) / (\n        self.hypercube_range[1] - self.hypercube_range[0]\n    )\n    epsilon = 1e-6\n    u = torch.clamp(u, epsilon, 1.0 - epsilon).double()\n\n    transformed_list = []\n    for i, prior in enumerate(self.priors):\n        dist_type = prior[0]\n        params = prior[1:]  # Support arbitrary number of parameters per prior\n        u_i = u[..., i]\n        x_i = self._forward_transform(u_i, dist_type, *params)\n        transformed_list.append(x_i)\n\n    return torch.stack(transformed_list, dim=-1)\n</code></pre>"},{"location":"api/hypercube/#falcon.priors.hypercube.Hypercube.inverse","title":"inverse","text":"<pre><code>inverse(x)\n</code></pre> <p>Applies the inverse transformation to a batch of values from the target distributions.</p> <p>The input tensor x should have shape (..., n_params). Each value is mapped back to [0,1] and then rescaled to the hypercube_range.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Tensor of shape (..., n_params) with values in the target distribution domains.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Tensor of shape (..., n_params) with values in the hypercube_range.</p> Source code in <code>falcon/priors/hypercube.py</code> <pre><code>def inverse(self, x):\n    \"\"\"\n    Applies the inverse transformation to a batch of values from the target distributions.\n\n    The input tensor x should have shape (..., n_params). Each value is mapped back to [0,1]\n    and then rescaled to the hypercube_range.\n\n    Args:\n        x (torch.Tensor): Tensor of shape (..., n_params) with values in the target distribution domains.\n\n    Returns:\n        torch.Tensor: Tensor of shape (..., n_params) with values in the hypercube_range.\n    \"\"\"\n    inv_list = []\n    for i, prior in enumerate(self.priors):\n        dist_type = prior[0]\n        params = prior[1:]\n        x_i = x[..., i]\n        u_i = self._inverse_transform(x_i, dist_type, *params)\n        inv_list.append(u_i)\n\n    u = torch.stack(inv_list, dim=-1)\n    u = (\n        u * (self.hypercube_range[1] - self.hypercube_range[0])\n        + self.hypercube_range[0]\n    )\n    return u\n</code></pre>"},{"location":"api/hypercube/#falcon.priors.hypercube.Hypercube.simulate_batch","title":"simulate_batch","text":"<pre><code>simulate_batch(batch_size)\n</code></pre> <p>Generates a batch of samples from the target distributions.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of samples to generate.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Tensor of shape (n_samples, n_params) with samples in the target distributions.</p> Source code in <code>falcon/priors/hypercube.py</code> <pre><code>def simulate_batch(self, batch_size):\n    \"\"\"\n    Generates a batch of samples from the target distributions.\n\n    Args:\n        batch_size (int): Number of samples to generate.\n\n    Returns:\n        torch.Tensor: Tensor of shape (n_samples, n_params) with samples in the target distributions.\n    \"\"\"\n    # Generate random samples in the hypercube_range\n    u = (\n        torch.rand(batch_size, len(self.priors))\n        * (self.hypercube_range[1] - self.hypercube_range[0])\n        + self.hypercube_range[0]\n    )\n    u = (\n        torch.rand(batch_size, len(self.priors), dtype=torch.float64)\n        * (self.hypercube_range[1] - self.hypercube_range[0])\n        + self.hypercube_range[0]\n    )\n    return self.forward(u).numpy()\n</code></pre>"},{"location":"api/product/","title":"Product Prior","text":"<p>Product of independent marginal distributions with latent space transformations.</p>"},{"location":"api/product/#overview","title":"Overview","text":"<p><code>Product</code> defines a prior as a product of independent 1D marginals, each with a bijective map to a chosen latent space. It extends the abstract base class <code>TransformedPrior</code>, which defines the <code>forward</code>/<code>inverse</code> interface.</p> <p><code>Product</code> supports two latent-space modes:</p> <ul> <li><code>\"hypercube\"</code>: Maps to/from a bounded hypercube (used with Flow)</li> <li><code>\"standard_normal\"</code>: Maps to/from standard normal space (used with Gaussian)</li> </ul>"},{"location":"api/product/#supported-distributions","title":"Supported Distributions","text":"Type Parameters Description <code>uniform</code> <code>low</code>, <code>high</code> Uniform distribution <code>normal</code> <code>mean</code>, <code>std</code> Gaussian distribution <code>cosine</code> <code>low</code>, <code>high</code> Cosine-weighted distribution <code>sine</code> <code>low</code>, <code>high</code> Sine-weighted distribution <code>uvol</code> <code>low</code>, <code>high</code> Uniform-in-volume <code>triangular</code> <code>a</code>, <code>c</code>, <code>b</code> Triangular distribution (min, mode, max) <code>lognormal</code> <code>mean</code>, <code>std</code> Log-normal distribution <code>fixed</code> <code>value</code> Fixed (non-inferred) parameter"},{"location":"api/product/#fixed-parameters","title":"Fixed Parameters","text":"<p>Use the <code>fixed</code> distribution type to hold a parameter constant. Fixed parameters are excluded from the inferred parameter space but still appear in the full parameter vector passed to the simulator:</p> <pre><code>simulator:\n  _target_: falcon.priors.Product\n  priors:\n    - ['normal', 0.0, 1.0]     # Inferred\n    - ['fixed', 3.14]           # Held constant\n    - ['uniform', -1.0, 1.0]   # Inferred\n</code></pre>"},{"location":"api/product/#usage","title":"Usage","text":"<pre><code>from falcon.priors import Product\n\nprior = Product(\n    priors=[\n        ('normal', 0.0, 1.0),\n        ('uniform', -10.0, 10.0),\n    ]\n)\n\n# Sample from prior\nsamples = prior.simulate_batch(1000)\n\n# Transform to/from latent space\nz = prior.inverse(samples, mode=\"standard_normal\")\nx = prior.forward(z, mode=\"standard_normal\")\n</code></pre>"},{"location":"api/product/#yaml-configuration","title":"YAML Configuration","text":""},{"location":"api/product/#with-flow-estimator","title":"With Flow estimator","text":"<pre><code>simulator:\n  _target_: falcon.priors.Product\n  priors:\n    - ['uniform', -100.0, 100.0]\n    - ['uniform', -100.0, 100.0]\n</code></pre>"},{"location":"api/product/#with-gaussian-estimator","title":"With Gaussian estimator","text":"<pre><code>simulator:\n  _target_: falcon.priors.Product\n  priors:\n    - ['normal', 0.0, 1.0]\n    - ['normal', 0.0, 1.0]\n</code></pre>"},{"location":"api/product/#class-reference","title":"Class Reference","text":""},{"location":"api/product/#falcon.priors.product.TransformedPrior","title":"TransformedPrior","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for priors that support latent space transformations.</p> <p>Subclasses must implement forward() and inverse() with a mode parameter:   - forward(z, mode): latent space -&gt; parameter space   - inverse(x, mode): parameter space -&gt; latent space</p> Modes <ul> <li>\"hypercube\": Maps to/from bounded hypercube. Use with Flow estimator.</li> <li>\"standard_normal\": Maps to/from N(0, I). Use with Gaussian estimator.</li> </ul> <p>This base class is used for type checking in estimators like Gaussian that require the transformation interface.</p>"},{"location":"api/product/#falcon.priors.product.TransformedPrior.param_dim","title":"param_dim  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>param_dim\n</code></pre> <p>Dimension of the parameter space.</p>"},{"location":"api/product/#falcon.priors.product.TransformedPrior.forward","title":"forward  <code>abstractmethod</code>","text":"<pre><code>forward(z, mode='hypercube')\n</code></pre> <p>Transform from latent space to parameter space.</p> Source code in <code>falcon/priors/product.py</code> <pre><code>@abstractmethod\ndef forward(self, z, mode: str = \"hypercube\"):\n    \"\"\"Transform from latent space to parameter space.\"\"\"\n    pass\n</code></pre>"},{"location":"api/product/#falcon.priors.product.TransformedPrior.inverse","title":"inverse  <code>abstractmethod</code>","text":"<pre><code>inverse(x, mode='hypercube')\n</code></pre> <p>Transform from parameter space to latent space.</p> Source code in <code>falcon/priors/product.py</code> <pre><code>@abstractmethod\ndef inverse(self, x, mode: str = \"hypercube\"):\n    \"\"\"Transform from parameter space to latent space.\"\"\"\n    pass\n</code></pre>"},{"location":"api/product/#falcon.priors.product.TransformedPrior.simulate_batch","title":"simulate_batch  <code>abstractmethod</code>","text":"<pre><code>simulate_batch(batch_size)\n</code></pre> <p>Sample from the prior distribution.</p> Source code in <code>falcon/priors/product.py</code> <pre><code>@abstractmethod\ndef simulate_batch(self, batch_size: int):\n    \"\"\"Sample from the prior distribution.\"\"\"\n    pass\n</code></pre>"},{"location":"api/product/#falcon.priors.product.Product","title":"Product","text":"<pre><code>Product(priors=[], hypercube_range=[-2, 2])\n</code></pre> <p>               Bases: <code>TransformedPrior</code></p> <p>Maps between target distributions and a latent space (hypercube or standard normal).</p> Supports bi-directional transformation with mode selection at call time <ul> <li>forward(z, mode): latent space -&gt; target distribution</li> <li>inverse(x, mode): target distribution -&gt; latent space</li> </ul> Modes <ul> <li>\"hypercube\": Maps to/from hypercube domain (default [-2, 2]). Use with Flow estimator.</li> <li>\"standard_normal\": Maps to/from N(0, I). Use with Gaussian estimator.</li> </ul> Supported distribution types and their required parameters <ul> <li>\"uniform\": Linear mapping. Parameters: low, high.</li> <li>\"cosine\": Uses acos transform for pdf \u221d sin(angle). Parameters: low, high.</li> <li>\"sine\": Uses asin transform. Parameters: low, high.</li> <li>\"uvol\": Uniform-in-volume. Parameters: low, high.</li> <li>\"normal\": Normal distribution. Parameters: mean, std.</li> <li>\"triangular\": Triangular distribution. Parameters: a (min), c (mode), b (max).</li> <li>\"fixed\": Fixed value (excluded from latent space). Parameters: value.</li> </ul> Example <p>prior = Product([     (\"uniform\", -100.0, 100.0),     (\"fixed\", 5.0),              # Fixed parameter, not in latent space     (\"normal\", 0.0, 1.0), ])</p> <p>Initialize Product.</p> <p>Parameters:</p> Name Type Description Default <code>priors</code> <p>List of tuples (dist_type, param1, param2, ...).</p> <code>[]</code> <code>hypercube_range</code> <p>Range for hypercube mode (default: [-2, 2]).</p> <code>[-2, 2]</code> Source code in <code>falcon/priors/product.py</code> <pre><code>def __init__(self, priors=[], hypercube_range=[-2, 2]):\n    \"\"\"\n    Initialize Product.\n\n    Args:\n        priors: List of tuples (dist_type, param1, param2, ...).\n        hypercube_range: Range for hypercube mode (default: [-2, 2]).\n    \"\"\"\n    self.priors = priors\n    self.hypercube_range = hypercube_range\n\n    # Separate fixed and free parameters\n    self._free_indices = []\n    self._fixed_indices = []\n    self._fixed_values = {}\n    for i, prior in enumerate(priors):\n        dist_type = prior[0]\n        if dist_type == \"fixed\":\n            self._fixed_indices.append(i)\n            self._fixed_values[i] = prior[1]\n        else:\n            self._free_indices.append(i)\n\n    self._param_dim = len(self._free_indices)  # Latent space dimension\n    self._full_param_dim = len(priors)  # Full output dimension\n</code></pre>"},{"location":"api/product/#falcon.priors.product.Product--latent-space-has-dim2-only-free-params","title":"Latent space has dim=2 (only free params)","text":""},{"location":"api/product/#falcon.priors.product.Product--output-space-has-dim3-includes-fixed-params","title":"Output space has dim=3 (includes fixed params)","text":""},{"location":"api/product/#falcon.priors.product.Product--for-gaussian-estimator-standard-normal-latent-space","title":"For Gaussian estimator (standard normal latent space)","text":"<p>z = prior.inverse(theta, mode=\"standard_normal\")  # theta: (..., 3) -&gt; z: (..., 2) theta = prior.forward(z, mode=\"standard_normal\")  # z: (..., 2) -&gt; theta: (..., 3)</p>"},{"location":"api/product/#falcon.priors.product.Product--for-flow-estimator-hypercube-latent-space","title":"For Flow estimator (hypercube latent space)","text":"<p>u = prior.inverse(theta, mode=\"hypercube\") theta = prior.forward(u, mode=\"hypercube\")</p>"},{"location":"api/product/#falcon.priors.product.Product.forward","title":"forward","text":"<pre><code>forward(z, mode='hypercube')\n</code></pre> <p>Map from latent space to target distribution.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <p>Tensor of shape (..., param_dim) in latent space (free params only).</p> required <code>mode</code> <p>\"hypercube\" or \"standard_normal\".</p> <code>'hypercube'</code> <p>Returns:</p> Type Description <p>Tensor of shape (..., full_param_dim) in target distribution space.</p> Source code in <code>falcon/priors/product.py</code> <pre><code>def forward(self, z, mode=\"hypercube\"):\n    \"\"\"\n    Map from latent space to target distribution.\n\n    Args:\n        z: Tensor of shape (..., param_dim) in latent space (free params only).\n        mode: \"hypercube\" or \"standard_normal\".\n\n    Returns:\n        Tensor of shape (..., full_param_dim) in target distribution space.\n    \"\"\"\n    # Handle case with no free parameters\n    if self._param_dim == 0:\n        batch_shape = z.shape[:-1] if z.dim() &gt; 1 else (1,)\n        result = torch.zeros(*batch_shape, self._full_param_dim, dtype=z.dtype, device=z.device)\n        for idx, val in self._fixed_values.items():\n            result[..., idx] = val\n        return result\n\n    if mode == \"standard_normal\":\n        # Try direct transforms first (avoids CDF precision issues at tails)\n        transformed = [None] * self._full_param_dim\n        use_direct = True\n        z_idx = 0\n        for i, prior in enumerate(self.priors):\n            dist_type, *params = prior\n            if dist_type == \"fixed\":\n                transformed[i] = torch.full(z.shape[:-1], params[0], dtype=z.dtype, device=z.device)\n            else:\n                x_i = self._from_standard_normal(z[..., z_idx], dist_type, *params)\n                if x_i is None:\n                    use_direct = False\n                    break\n                transformed[i] = x_i\n                z_idx += 1\n        if use_direct:\n            return torch.stack(transformed, dim=-1)\n        # Fall through to CDF approach if any distribution lacks direct transform\n        u = self._normal_to_uniform(z)\n    elif mode == \"hypercube\":\n        u = self._hypercube_to_uniform(z)\n    else:\n        raise ValueError(f\"Unknown mode: {mode}. Use 'hypercube' or 'standard_normal'.\")\n\n    # Map [0, 1] to target distributions (CDF approach)\n    epsilon = 1e-10  # Supports ~6 sigma tails in float64\n    u = torch.clamp(u, epsilon, 1.0 - epsilon)\n\n    transformed = []\n    u_idx = 0\n    for i, prior in enumerate(self.priors):\n        dist_type, *params = prior\n        if dist_type == \"fixed\":\n            x_i = torch.full(u.shape[:-1], params[0], dtype=u.dtype, device=u.device)\n        else:\n            x_i = self._forward_transform(u[..., u_idx], dist_type, *params)\n            u_idx += 1\n        transformed.append(x_i)\n\n    return torch.stack(transformed, dim=-1)\n</code></pre>"},{"location":"api/product/#falcon.priors.product.Product.inverse","title":"inverse","text":"<pre><code>inverse(x, mode='hypercube')\n</code></pre> <p>Map from target distribution to latent space.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <p>Tensor of shape (..., full_param_dim) in target distribution space.</p> required <code>mode</code> <p>\"hypercube\" or \"standard_normal\".</p> <code>'hypercube'</code> <p>Returns:</p> Type Description <p>Tensor of shape (..., param_dim) in latent space (free params only).</p> Source code in <code>falcon/priors/product.py</code> <pre><code>def inverse(self, x, mode=\"hypercube\"):\n    \"\"\"\n    Map from target distribution to latent space.\n\n    Args:\n        x: Tensor of shape (..., full_param_dim) in target distribution space.\n        mode: \"hypercube\" or \"standard_normal\".\n\n    Returns:\n        Tensor of shape (..., param_dim) in latent space (free params only).\n    \"\"\"\n    # Handle case with no free parameters\n    if self._param_dim == 0:\n        batch_shape = x.shape[:-1] if x.dim() &gt; 1 else (1,)\n        return torch.zeros(*batch_shape, 0, dtype=x.dtype, device=x.device)\n\n    if mode == \"standard_normal\":\n        # Try direct transforms first (avoids CDF precision issues at tails)\n        transformed = []\n        use_direct = True\n        for i, prior in enumerate(self.priors):\n            dist_type, *params = prior\n            if dist_type == \"fixed\":\n                continue  # Skip fixed parameters\n            z_i = self._to_standard_normal(x[..., i], dist_type, *params)\n            if z_i is None:\n                use_direct = False\n                break\n            transformed.append(z_i)\n        if use_direct:\n            return torch.stack(transformed, dim=-1)\n        # Fall through to CDF approach if any distribution lacks direct transform\n\n    # Map target distributions to [0, 1] (CDF approach, free params only)\n    uniform = []\n    for i, prior in enumerate(self.priors):\n        dist_type, *params = prior\n        if dist_type == \"fixed\":\n            continue  # Skip fixed parameters\n        u_i = self._inverse_transform(x[..., i], dist_type, *params)\n        uniform.append(u_i)\n\n    u = torch.stack(uniform, dim=-1)\n\n    # Clamp to avoid numerical issues at boundaries\n    epsilon = 1e-10  # Supports ~6 sigma tails in float64\n    u = torch.clamp(u, epsilon, 1.0 - epsilon)\n\n    # Convert [0, 1] to latent space\n    if mode == \"hypercube\":\n        return self._uniform_to_hypercube(u)\n    elif mode == \"standard_normal\":\n        return self._uniform_to_normal(u)\n    else:\n        raise ValueError(f\"Unknown mode: {mode}. Use 'hypercube' or 'standard_normal'.\")\n</code></pre>"},{"location":"api/product/#falcon.priors.product.Product.simulate_batch","title":"simulate_batch","text":"<pre><code>simulate_batch(batch_size)\n</code></pre> <p>Generate samples from the target distributions.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <p>Number of samples.</p> required <p>Returns:</p> Type Description <p>numpy array of shape (batch_size, full_param_dim) in target distribution space.</p> Source code in <code>falcon/priors/product.py</code> <pre><code>def simulate_batch(self, batch_size):\n    \"\"\"\n    Generate samples from the target distributions.\n\n    Args:\n        batch_size: Number of samples.\n\n    Returns:\n        numpy array of shape (batch_size, full_param_dim) in target distribution space.\n    \"\"\"\n    # Sample uniform for free parameters only\n    u = torch.rand(batch_size, self._param_dim, dtype=torch.float64)\n\n    transformed = []\n    u_idx = 0\n    for i, prior in enumerate(self.priors):\n        dist_type, *params = prior\n        if dist_type == \"fixed\":\n            x_i = torch.full((batch_size,), params[0], dtype=torch.float64)\n        else:\n            x_i = self._forward_transform(u[..., u_idx], dist_type, *params)\n            u_idx += 1\n        transformed.append(x_i)\n\n    return torch.stack(transformed, dim=-1).numpy()\n</code></pre>"}]}